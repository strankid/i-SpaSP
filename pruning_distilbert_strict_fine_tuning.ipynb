{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tNXnlzDYycri",
        "U-l0PRfFyenw",
        "bkTKpTdEQPaF",
        "VdryzDILQYua",
        "8EN5SnK6QiTb",
        "K0mGPnklQ6RB",
        "Mzzs5h5eRbBZ",
        "1il8obD1Ru_C",
        "5uzqg4OgAbEY",
        "TegeTJT-AYkN",
        "Bj7kwd_xylNW",
        "qI7fIncYepgW",
        "OStkavXOypi-",
        "E3VcmNnJ3Ce7",
        "2WKY_r5_yic0"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "093a267d82a148aa88cab8678ef40c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb0a98a8ecab490ea38ddeb9e1a56c9c",
              "IPY_MODEL_c2004c50cc1c49528da19a93aa719e6f",
              "IPY_MODEL_0a0ac276c31c46cdb3e253e384a0dd2f"
            ],
            "layout": "IPY_MODEL_890fc16734c04e28ba16a22aabdb56d1"
          }
        },
        "cb0a98a8ecab490ea38ddeb9e1a56c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7750926384a4e4c8136fd960c162ed1",
            "placeholder": "​",
            "style": "IPY_MODEL_38a5e2749a9845cbbadb5b604a5159d6",
            "value": "Running tokenizer on train dataset: 100%"
          }
        },
        "c2004c50cc1c49528da19a93aa719e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bbc79c16c534c0cbce991b53fe2f5a8",
            "max": 15000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbdc1a978e5d47ae9074e31496edf375",
            "value": 15000
          }
        },
        "0a0ac276c31c46cdb3e253e384a0dd2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4e34dd4fa0b412ca326f5e89c986d6f",
            "placeholder": "​",
            "style": "IPY_MODEL_d9d15efaeae44145b9a8e1558522bb31",
            "value": " 15000/15000 [00:36&lt;00:00, 456.86 examples/s]"
          }
        },
        "890fc16734c04e28ba16a22aabdb56d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7750926384a4e4c8136fd960c162ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38a5e2749a9845cbbadb5b604a5159d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bbc79c16c534c0cbce991b53fe2f5a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbdc1a978e5d47ae9074e31496edf375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4e34dd4fa0b412ca326f5e89c986d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9d15efaeae44145b9a8e1558522bb31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "tNXnlzDYycri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-oHq9Kby0PX",
        "outputId": "c34e92bf-bbbb-4572-fcda-4eaedd060787"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.2-py3-none-any.whl (518 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, safetensors, xxhash, dill, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate\n",
            "Successfully installed datasets-2.14.2 dill-0.3.7 evaluate-0.4.0 huggingface-hub-0.16.4 multiprocess-0.70.15 responses-0.18.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJh-pxJVy1l0",
        "outputId": "eab62d65-7419-4389-bdc1-a9ee5376f09a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "import math\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from copy import deepcopy\n",
        "import time\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    DistilBertForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions\n",
        ")\n",
        "from transformers.modeling_utils import apply_chunking_to_forward\n",
        "from transformers.trainer_utils import SchedulerType\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version\n",
        "from transformers.utils.versions import require_version\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "EXO_WWgIxjcj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_min_version(\"4.16.0.dev0\")\n",
        "\n",
        "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "puCM_-l8xmM0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training argument classes"
      ],
      "metadata": {
        "id": "U-l0PRfFyenw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    Using `HfArgumentParser` we can turn this class\n",
        "    into argparse arguments to be able to specify them on\n",
        "    the command line.\n",
        "    \"\"\"\n",
        "\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
        "    )\n",
        "    pad_to_max_length: bool = field(\n",
        "        default=True,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
        "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n",
        "    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})"
      ],
      "metadata": {
        "id": "QE-6KkbLxpc4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    language: str = field(\n",
        "        default=None, metadata={\"help\": \"Evaluation language. Also train language if `train_language` is set to None.\"}\n",
        "    )\n",
        "    train_language: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Train language if it is different from the evaluation language.\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    do_lower_case: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
        "            \"with private models).\"\n",
        "        },\n",
        "    )"
      ],
      "metadata": {
        "id": "hyvzGLGXxtQI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkTKpTdEQPaF"
      },
      "source": [
        "# pruning algorithm steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdryzDILQYua"
      },
      "source": [
        "### step 1: compute gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "cH_MvkuUQTjJ"
      },
      "outputs": [],
      "source": [
        "def compute_grads(args, dense_out_lin, pruned_out_lin, H, weight_indexer, device='cpu'):\n",
        "  dense_out_lin = dense_out_lin.to(device)\n",
        "  U = dense_out_lin(H)\n",
        "\n",
        "  if weight_indexer is not None:\n",
        "    with torch.no_grad():\n",
        "      U_pruned = pruned_out_lin(H[:, :, weight_indexer])\n",
        "    loss = residual_objective(U - U_pruned.detach())\n",
        "  else:\n",
        "    loss = residual_objective(U)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  dW = dense_out_lin.weight.grad\n",
        "\n",
        "  # maybe instead of using attn grad, should aggregate parameter grads? either for weights only or for all\n",
        "  pruned_out_lin.zero_grad()\n",
        "  dense_out_lin.zero_grad()\n",
        "  H.grad = None\n",
        "  U.grad = None\n",
        "\n",
        "  return dW, loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EN5SnK6QiTb"
      },
      "source": [
        "### step 2: find best s columns of grad outside S and merge with S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "KEWVnQeLQhkD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def find_and_merge(dW, S, n_heads_og, dim_per_head, n_heads_to_keep, device='cpu'):\n",
        "  dW_by_head = dW.view(dW.shape[0], n_heads_og, dim_per_head).to(device)\n",
        "  importance = torch.norm(dW_by_head, p=2, dim=(0, 2))\n",
        "\n",
        "  for index in S:\n",
        "    importance[index] = 0\n",
        "  imp_top_idxs = torch.argsort(importance, descending=True)[:n_heads_to_keep]\n",
        "  imp_top_idxs = set(imp_top_idxs.tolist())\n",
        "\n",
        "  D = S.union(imp_top_idxs)\n",
        "\n",
        "  return D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0mGPnklQ6RB"
      },
      "source": [
        "### step 3: update parameters by gradient descent focused on D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "SWNeUbvIQ5zD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def update_step(args, pruned_out_lin, dense_out_lin, Q, dW, D, S_indexer, n_heads, attention_head_size, H, device=None):\n",
        "  D_not = set(range(n_heads)).difference(D)\n",
        "  not_D_indexer = get_weight_indexer(D_not, attention_head_size)\n",
        "  dW_at_D_t = deepcopy(dW)\n",
        "  if not args.dense_update:\n",
        "    dW_at_D_t[:, not_D_indexer] = 0\n",
        "\n",
        "  if not args.maintain_Q:\n",
        "    Q = torch.zeros_like(dW)\n",
        "    if S_indexer is not None:\n",
        "      Q[:, S_indexer] = pruned_out_lin.weight.data\n",
        "    else:\n",
        "      Q = deepcopy(dense_out_lin.weight.data)\n",
        "\n",
        "  Q_by_head = Q.view(Q.shape[0], n_heads, attention_head_size).to(device)\n",
        "  Q_imp = torch.norm(Q_by_head, p=2, dim=(0, 2)) # get the importance of each head\n",
        "  # print(f'before update Q_imp: {Q_imp}')\n",
        "\n",
        "  if args.eta == 'adaptive':\n",
        "     eta = torch.norm(dW_at_D_t) ** 2 / torch.norm(H @ dW_at_D_t.T) ** 2\n",
        "  else:\n",
        "    eta = args.eta\n",
        "\n",
        "  # print(f'update grad * eta: {eta * dW_at_D_t}')\n",
        "  Q = Q - eta * dW_at_D_t\n",
        "\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzzs5h5eRbBZ"
      },
      "source": [
        "### step 4: truncate Q to be s-sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "rWLNhMImRgwn"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def truncate(Q, pruned_out_lin, n_heads_og, dim_per_head, n_heads_to_keep, device='cpu'):\n",
        "  Q_by_head = Q.view(Q.shape[0], n_heads_og, dim_per_head).to(device)\n",
        "\n",
        "  Q_imp = torch.norm(Q_by_head, p=2, dim=(0, 2)) # get the importance of each head\n",
        "  # print(f'Q_imp: {Q_imp}')\n",
        "  imp_top_idxs = torch.argsort(Q_imp, descending=True)[:n_heads_to_keep]\n",
        "  S = set(imp_top_idxs.cpu().tolist())\n",
        "\n",
        "  S_indexer = get_weight_indexer(S, dim_per_head)\n",
        "  pruned_out_lin.weight.data = Q[:, S_indexer]\n",
        "\n",
        "  return pruned_out_lin, S, S_indexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1il8obD1Ru_C"
      },
      "source": [
        "### step 5: debias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "OvcRPgoWRxNg"
      },
      "outputs": [],
      "source": [
        "def debias(args, pruned_out_lin, dense_out_lin, Q, H, S_indexer, iters, device='cpu'):\n",
        "  H_pruned = H[:, :, S_indexer]\n",
        "  for i in range(iters):\n",
        "    # print(f'debias iter {i}')\n",
        "    pruned_out_lin.zero_grad()\n",
        "    dense_out_lin.zero_grad()\n",
        "\n",
        "    if args.debias_dense:\n",
        "      U_dense = dense_out_lin(H)\n",
        "      with torch.no_grad():\n",
        "        U_pruned = pruned_out_lin(H_pruned)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        U_dense = dense_out_lin(H)\n",
        "      U_pruned = pruned_out_lin(H_pruned)\n",
        "\n",
        "    if args.debias_dense:\n",
        "      loss = residual_objective(U_dense - U_pruned.detach())\n",
        "    else:\n",
        "      loss = residual_objective(U_dense.detach() - U_pruned)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      if args.debias_dense:\n",
        "        dW = dense_out_lin.weight.grad\n",
        "        if args.eta == 'adaptive':\n",
        "          eta = torch.norm(dW) ** 2 / torch.norm(H @ dW.T) ** 2\n",
        "        else:\n",
        "          eta = args.eta\n",
        "\n",
        "        Q = Q - eta * dW\n",
        "        pruned_out_lin.weight.data = Q[:, S_indexer]\n",
        "\n",
        "      else:\n",
        "        dW_pruned = pruned_out_lin.weight.grad\n",
        "\n",
        "        if args.eta == 'adaptive':\n",
        "          eta = torch.norm(dW_pruned) ** 2 / torch.norm(H_pruned @ dW_pruned.T) ** 2\n",
        "        else:\n",
        "          eta = args.eta\n",
        "\n",
        "        # print(f'debias grad: {dW_pruned}')\n",
        "        # print(f'debias weight: {pruned_out_lin.weight.data}')\n",
        "\n",
        "        pruned_out_lin.weight.data = pruned_out_lin.weight.data - eta * dW_pruned\n",
        "        Q[:, S_indexer] = pruned_out_lin.weight.data\n",
        "\n",
        "  return pruned_out_lin, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "iIF7YP0Hyni3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## general"
      ],
      "metadata": {
        "id": "0YrHHG_XAXOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dotdict:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            if isinstance(value, dict):\n",
        "                setattr(self, key, dotdict(value))\n",
        "            else:\n",
        "                setattr(self, key, value)"
      ],
      "metadata": {
        "id": "z45lWKWc5bIL"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrn(ckpt_path, model):\n",
        "    print(f'\\n\\nLoading model from: {ckpt_path}\\n\\n')\n",
        "    #CONFIG_NAME = 'config.json'\n",
        "    WEIGHTS_NAME = 'pytorch_model.bin'\n",
        "    #cfg_path = os.path.join(ckpt_path, CONFIG_NAME)\n",
        "    weight_path = os.path.join(ckpt_path, WEIGHTS_NAME)\n",
        "    assert os.path.exists(weight_path)\n",
        "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state_dict)\n",
        "    del state_dict\n",
        "    return model"
      ],
      "metadata": {
        "id": "gC7CduCAyFDj"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transformer_arguments(model, data, device='cpu'):\n",
        "  model = model.to(device)\n",
        "  input_ids = data['input_ids'].to(device)\n",
        "  with torch.no_grad():\n",
        "    embeddings = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "  input_shape = input_ids.size()\n",
        "\n",
        "  attention_mask = data['attention_mask']\n",
        "  if attention_mask is None:\n",
        "      attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
        "\n",
        "  return embeddings, attention_mask"
      ],
      "metadata": {
        "id": "uNxYtU8b1OvE"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pruning"
      ],
      "metadata": {
        "id": "5uzqg4OgAbEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate heads\n",
        "def separate_heads(x, bs, n_heads, dim_per_head):\n",
        "  return x.view(bs, -1, n_heads, dim_per_head).transpose(1, 2)"
      ],
      "metadata": {
        "id": "7Uwxumfi2mdJ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight_indexer(head_list, head_dim):\n",
        "  indices = []\n",
        "  for i in head_list:\n",
        "      indices.extend(list(range(head_dim*i, head_dim*(i + 1))))\n",
        "  indices = sorted(indices)\n",
        "  return torch.LongTensor(indices)"
      ],
      "metadata": {
        "id": "Gq4wCvvn2lFZ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_objective(mat):\n",
        "    return 0.5 * torch.norm(mat) # using torch.sum requires small stepsize (order of 1e-5)"
      ],
      "metadata": {
        "id": "sAU4XOtW2kNr"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(model, data):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    iids = data['input_ids'].to(device)\n",
        "    ttids = data['token_type_ids'].to(device)\n",
        "    att_mask = data['attention_mask'].to(device)\n",
        "    ext_att_mask = model.bert.get_extended_attention_mask(att_mask, iids.size(), device)\n",
        "    #head_mask = model.bert.get_head_mask(None, model.bert.config.num_hidden_layers)\n",
        "    embedding_output = model.bert.embeddings(input_ids=iids, token_type_ids=ttids)\n",
        "    return embedding_output, ext_att_mask"
      ],
      "metadata": {
        "id": "RbkusaY4zEB6"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "TegeTJT-AYkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples, tokenizer):\n",
        "    # Tokenize the texts\n",
        "    inputs = tokenizer(\n",
        "      examples['question'],\n",
        "      examples[\"context\"],\n",
        "      max_length=512,\n",
        "      truncation=\"only_second\",\n",
        "      stride=128,\n",
        "      padding=\"max_length\",\n",
        "      return_tensors='pt',\n",
        "      return_offsets_mapping=True,\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "wwfZagaK43tE"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# finetune"
      ],
      "metadata": {
        "id": "Bj7kwd_xylNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(args, model, train_dict, val_arguments, epochs, save_path=None, save_model=False, lr=5e-5, lr_decay=True, device='cpu'):\n",
        "    train_dict['args'].learning_rate = lr\n",
        "    if lr_decay:\n",
        "        train_dict['args'].lr_scheduler_type = SchedulerType.LINEAR\n",
        "    else:\n",
        "        train_dict['args'].lr_scheduler_type = SchedulerType.CONSTANT\n",
        "    train_dict['args'].num_train_epochs = float(epochs)\n",
        "    trainer = Trainer(model=model, **train_dict)\n",
        "    model.train()\n",
        "    train_result = trainer.train(resume_from_checkpoint=None)\n",
        "    metrics = train_result.metrics\n",
        "    max_train_samples = len(train_dict['train_dataset'])\n",
        "    metrics[\"train_samples\"] = max_train_samples\n",
        "    if save_path is not None:\n",
        "        trainer.output_dir = save_path\n",
        "        trainer.run_name = save_path\n",
        "        if save_model:\n",
        "            trainer.save_model()\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "\n",
        "    model.eval()\n",
        "    val_arguments['model'] = model\n",
        "    metrics = validate(args, **val_arguments, device=device)\n",
        "    print(f'after finetuning: {metrics}')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "WJ2164FJyGnz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validate"
      ],
      "metadata": {
        "id": "qI7fIncYepgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(args, model, ids, inputs, answers, metric, tokenizer, device='cpu'):\n",
        "  model = model.to(device)\n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad():\n",
        "    start_sp = time.time()\n",
        "    outputs_sp = model(**inputs)\n",
        "    end_sp = time.time()\n",
        "    if args.verbose:\n",
        "      print(f'done predicting using sparse model. time elapsed = {end_sp - start_sp}s')\n",
        "\n",
        "  num_examples = len(ids)\n",
        "  preds = []\n",
        "  refs = []\n",
        "  for i in range(num_examples):\n",
        "    answer_start_index_sp = torch.argmax(outputs_sp.start_logits[i])\n",
        "    answer_end_index_sp = torch.argmax(outputs_sp.end_logits[i])\n",
        "    predict_answer_tokens_sp = inputs.input_ids[i, answer_start_index_sp : answer_end_index_sp + 1]\n",
        "    pred_sp = tokenizer.decode(predict_answer_tokens_sp)\n",
        "\n",
        "    pred = {'id': ids[i], 'prediction_text': pred_sp}\n",
        "    preds.append(pred)\n",
        "    ref = {'answers': answers[i], 'id': ids[i]}\n",
        "    refs.append(ref)\n",
        "\n",
        "  results = metric.compute(predictions=preds, references=refs)\n",
        "  return results"
      ],
      "metadata": {
        "id": "JBp0XKExQ7Wz"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_val_arguments(args, val_ds, model, tokenizer):\n",
        "      val_arguments = {}\n",
        "      start_idx = torch.randint(low=0, high=len(val_ds['id']) - args.val_size, size=(1,)).item()\n",
        "\n",
        "      val_ids = val_ds['id'][start_idx : start_idx + args.val_size]\n",
        "      val_questions =  val_ds['question'][start_idx : start_idx + args.val_size]\n",
        "      val_texts =  val_ds['context'][start_idx : start_idx + args.val_size]\n",
        "      val_answers =  val_ds['answers'][start_idx : start_idx + args.val_size]\n",
        "\n",
        "      val_inputs = tokenizer(\n",
        "            val_questions,\n",
        "            val_texts,\n",
        "            max_length=512,\n",
        "            truncation=\"only_second\",\n",
        "            stride=128,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors='pt'\n",
        "      )\n",
        "\n",
        "      val_arguments['ids'] = val_ids\n",
        "      val_arguments['inputs'] = val_inputs\n",
        "      val_arguments['answers'] = val_answers\n",
        "      val_arguments['model'] = deepcopy(model)\n",
        "      val_arguments['metric'] = evaluate.load(args.dataset_name)\n",
        "      val_arguments['tokenizer'] = tokenizer\n",
        "\n",
        "      return val_arguments"
      ],
      "metadata": {
        "id": "A-77DUb7_McO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# comparisons"
      ],
      "metadata": {
        "id": "OStkavXOypi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_layer_random_prune(blay, prune_ratio=0.5):\n",
        "    satt = blay.attention.self\n",
        "    num_heads_to_prune = int((1 - prune_ratio)*satt.num_attention_heads)\n",
        "    heads_to_prune = list(range(satt.num_attention_heads))\n",
        "    random.shuffle(heads_to_prune)\n",
        "    heads_to_prune = heads_to_prune[:num_heads_to_prune]\n",
        "    blay.attention.prune_heads(heads_to_prune)\n",
        "    return blay"
      ],
      "metadata": {
        "id": "WwEvzk73yUHJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_global_masking(model, dl, num_batches=10, ratio=0.5):\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    num_layers = len(model.bert.encoder.layer)\n",
        "    num_heads = model.bert.encoder.layer[0].attention.self.num_attention_heads\n",
        "    final_num_heads = int(ratio * num_layers * num_heads)\n",
        "    mask_tensor = torch.ones(int(num_heads*num_layers))\n",
        "    mask_tensor = mask_tensor.to(device)\n",
        "    mask_tensor.requires_grad = True\n",
        "    for b in range(num_batches):\n",
        "        data_in = next(iter(dl))\n",
        "        labels = data_in['labels']\n",
        "        labels = labels.to(device)\n",
        "        hidden_rep, ext_att_mask = get_bert_embeddings(model, data_in)\n",
        "\n",
        "        # pass data thru each layer of the model\n",
        "        for i in range(num_layers):\n",
        "            blay = model.bert.encoder.layer[i]\n",
        "            satt = blay.attention.self\n",
        "            sout = blay.attention.output\n",
        "\n",
        "            query_out = satt.transpose_for_scores(satt.query(hidden_rep))\n",
        "            key_out = satt.transpose_for_scores(satt.key(hidden_rep))\n",
        "            value_out = satt.transpose_for_scores(satt.value(hidden_rep))\n",
        "\n",
        "            att_sc = torch.matmul(query_out, key_out.transpose(-1, -2)) # [32, 12, 128, 128], has all heads separated\n",
        "            att_sc = att_sc / math.sqrt(satt.attention_head_size)\n",
        "            att_sc += ext_att_mask # contains a bunch of negative infinities to remove stuff in softmax computation\n",
        "            att_prob = torch.nn.functional.softmax(att_sc, dim=-1)\n",
        "\n",
        "            ctxt = torch.matmul(att_prob, value_out) # [32, 12, 128, 64] still separated btwn heads\n",
        "            ctxt = ctxt.permute(0, 2, 1, 3).contiguous() # [32, 128, 12, 64]\n",
        "            sense_mask = mask_tensor[num_heads*i: num_heads*(i + 1)]\n",
        "\n",
        "            ctxt = ctxt * sense_mask[None, None, :, None] # add mask into the forward pass\n",
        "\n",
        "            new_shape = ctxt.size()[:-2] + (satt.all_head_size,)\n",
        "            ctxt = ctxt.view(*new_shape) # [32, 128, 768], THIS IS THE OUTPUT OF SELF ATTN\n",
        "\n",
        "            out = sout.dense(ctxt)\n",
        "            out = sout.LayerNorm(out + hidden_rep)\n",
        "\n",
        "            hidden_rep = apply_chunking_to_forward(blay.feed_forward_chunk,\n",
        "                    blay.chunk_size_feed_forward, blay.seq_len_dim, out)\n",
        "\n",
        "        output = BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=hidden_rep,\n",
        "                past_key_values=None, hidden_states=None, attentions=None,\n",
        "                cross_attentions=None)\n",
        "        pooled_output = model.bert.pooler(output[0])\n",
        "        pooled_output_wrapper = BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "                last_hidden_state=output[0], pooler_output=pooled_output,\n",
        "                past_key_values=None, hidden_states=None, attentions=None,\n",
        "                cross_attentions=None)\n",
        "        pool_out = pooled_output_wrapper[1]\n",
        "        logits = model.classifier(pool_out)\n",
        "        loss = loss_fn(logits.view(-1, model.num_labels), labels.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "    agg_grad = mask_tensor.grad.detach().cpu()\n",
        "    with torch.no_grad():\n",
        "        agg_grad = torch.abs(agg_grad)\n",
        "    imp_idxs = set(torch.argsort(agg_grad, descending=True)[:final_num_heads].cpu().tolist())\n",
        "    for l in range(num_layers):\n",
        "        heads_to_prune = []\n",
        "        for hi in range(num_heads):\n",
        "             curr_idx = num_heads * l + hi\n",
        "             if not curr_idx in imp_idxs:\n",
        "                 heads_to_prune.append(hi)\n",
        "        assert len(heads_to_prune) < num_heads\n",
        "        model.bert.encoder.layer[l].attention.prune_heads(heads_to_prune)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "IGDNa_9gyVQS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prune model"
      ],
      "metadata": {
        "id": "XbdADMgGytn6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "xFsFSVBAxdKJ"
      },
      "outputs": [],
      "source": [
        "def prune_distilbert(args, model, dl, num_batches, num_iter, ratios,\n",
        "        training_dict, val_arguments, per_layer_epochs, final_epochs, prune_type='ispasp'):\n",
        "\n",
        "    # performs i-SpaSP pruning on the distilBERT model for squad\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # get the output dir\n",
        "    save_base = training_dict['args'].output_dir\n",
        "\n",
        "    transformer = model.distilbert.transformer\n",
        "    layers = transformer.layer\n",
        "\n",
        "    metrics = validate(args, **val_arguments, device=device)\n",
        "    print(f'Pre-pruning metrics = {metrics}')\n",
        "    # start passing thru each layer of encoder\n",
        "    if prune_type in ['random', 'ispasp']:\n",
        "        for i, rat in enumerate(ratios):\n",
        "            if prune_type == 'ispasp':\n",
        "                data_list = []\n",
        "                mask_list = []\n",
        "                for b in range(num_batches):\n",
        "                    data_in = next(iter(dl))\n",
        "                    with torch.no_grad():\n",
        "                        model.eval()\n",
        "                        hidden_rep, ext_att_mask = get_transformer_arguments(model, data_in, device)\n",
        "                        # hidden_rep, ext_att_mask = get_bert_embeddings(model, data_in)\n",
        "                        for j in range(i):\n",
        "                            hidden_rep = layers[j](hidden_rep, ext_att_mask)[0]\n",
        "                            #hidden_rep = bert_layer_forward(model.bert.encoder.layer[j],\n",
        "                            #        hidden_rep, ext_att_mask)\n",
        "                        data_list.append(hidden_rep.detach().cpu())\n",
        "                        mask_list.append(ext_att_mask.detach().cpu())\n",
        "                layers[i].attention = prune_attn_layer(args, layers[i].attention, data_list, mask_list, val_arguments, prune_ratio=rat, layer_id=i, device=device)\n",
        "\n",
        "            elif prune_type == 'random':\n",
        "                layers[i] = bert_layer_random_prune(\n",
        "                        layers[i], prune_ratio=rat)\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "            output_dir = os.path.join(save_base, f'prune_layer_{i}/')\n",
        "            model = train_and_eval(args, model, training_dict, val_arguments, per_layer_epochs,\n",
        "                    save_path=output_dir, save_model=False, lr=5e-5, lr_decay=True, device=device)\n",
        "\n",
        "        model = train_and_eval(args, model, training_dict, val_arguments, final_epochs,\n",
        "                save_path=save_base, save_model=True, lr=5e-5, lr_decay=True, device=device)\n",
        "\n",
        "    elif prune_type == 'masking':\n",
        "        model = prune_global_masking(model, dl, num_batches=num_batches, ratio=ratios[0])\n",
        "        model = train_and_eval(args, model, training_dict, val_arguments, final_epochs,\n",
        "                save_path=save_base, save_model=True, lr=5e-5, lr_decay=True, device=device)\n",
        "    else:\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prune layer"
      ],
      "metadata": {
        "id": "E3VcmNnJ3Ce7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_attn_layer(args, layer, hidden_states_list, mask_list, val_arguments, prune_ratio=0.5, layer_id=None, device='cpu'):\n",
        "  n_heads_to_keep = int(layer.n_heads * prune_ratio)\n",
        "\n",
        "  def shape(x):\n",
        "    \"\"\"separate heads\"\"\"\n",
        "    return x.view(x.shape[0], -1, layer.n_heads, layer.attention_head_size).transpose(1, 2)\n",
        "\n",
        "  def unshape(x):\n",
        "      \"\"\"group heads\"\"\"\n",
        "      return x.transpose(1, 2).contiguous().view(x.shape[0], -1, layer.n_heads * layer.attention_head_size)\n",
        "\n",
        "  dense_out_lin = layer.out_lin.to(device)\n",
        "  pruned_out_lin = nn.Linear(in_features = layer.attention_head_size * n_heads_to_keep,\n",
        "                             out_features = layer.dim,\n",
        "                             bias = True).to(device)\n",
        "  Q = deepcopy(dense_out_lin.weight.data)\n",
        "\n",
        "  S = set([]) # heads to keep\n",
        "  S_indexer = None\n",
        "\n",
        "  layer = layer.to(device)\n",
        "\n",
        "  hidden_states = torch.cat(hidden_states_list, dim=0)\n",
        "  attn_mask = torch.cat(mask_list, dim=0)\n",
        "  with torch.no_grad():\n",
        "      attn_mask = attn_mask.to(device)\n",
        "      hidden_states = hidden_states.to(device)\n",
        "      query, key, value = hidden_states, hidden_states, hidden_states\n",
        "\n",
        "      _, weights = layer(query, key, value, attn_mask, head_mask=None, output_attentions=True)\n",
        "\n",
        "      v = shape(layer.v_lin(value))\n",
        "      context = torch.matmul(weights, v)\n",
        "      H = unshape(context)\n",
        "\n",
        "  for t in range(args.iters):\n",
        "    if args.validate_iter:\n",
        "      val_arguments['model'].distilbert.transformer.layer[layer_id].attention.out_lin = pruned_out_lin # might not work with dimensions\n",
        "      acc = validate(args, **val_arguments, device=device)\n",
        "      pruned_out_lin = pruned_out_lin.to(device)\n",
        "\n",
        "    dW, loss = compute_grads(args, dense_out_lin, pruned_out_lin, H, S_indexer, device=device)\n",
        "\n",
        "    if args.validate_iter:\n",
        "      print(f'Iteration {t}: S = {S} | Loss = {loss} | Exact Match = {acc[\"exact_match\"]} | F1 = {acc[\"f1\"]}')\n",
        "    elif args.iter_verbose:\n",
        "      print(f'Iteration {t}: S = {S} | Loss = {loss}')\n",
        "\n",
        "    D = find_and_merge(dW, S, layer.n_heads, layer.attention_head_size, n_heads_to_keep, device=device)\n",
        "    Q = update_step(args, pruned_out_lin, dense_out_lin, Q, dW, D, S_indexer, layer.n_heads, layer.attention_head_size, H, device=None)\n",
        "    pruned_out_lin, S, S_indexer = truncate(Q, pruned_out_lin, layer.n_heads, layer.attention_head_size, n_heads_to_keep, device=device)\n",
        "    pruned_out_lin, Q = debias(args, pruned_out_lin, dense_out_lin, Q, H, S_indexer, args.debias_iters, device=device)\n",
        "\n",
        "  print(f'Post-pruning: S = {S}')\n",
        "  all_heads = set(range(layer.n_heads))\n",
        "\n",
        "  layer.prune_heads(all_heads.difference(S))\n",
        "  layer.out_lin = pruned_out_lin\n",
        "\n",
        "  if layer_id is not None:\n",
        "    print(f'Done pruning layer {layer_id}')\n",
        "\n",
        "  return layer"
      ],
      "metadata": {
        "id": "tT-9ixBXzs_D"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "2WKY_r5_yic0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    model_args = ModelArguments(tokenizer_name=args.tokenizer_name,\n",
        "                                model_name_or_path=args.model_name)\n",
        "\n",
        "    data_args = DataTrainingArguments()\n",
        "\n",
        "    training_args = TrainingArguments(output_dir=args.output_dir,\n",
        "                                      do_train=True,\n",
        "                                      do_eval=False)\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "\n",
        "    log_level = training_args.get_process_log_level()\n",
        "    logger.setLevel(log_level)\n",
        "    datasets.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.enable_default_handler()\n",
        "    transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "    # Log on each process the small summary:\n",
        "    logger.warning(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu} \"\n",
        "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Detecting last checkpoint.\n",
        "    last_checkpoint = None\n",
        "\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    if training_args.do_train:\n",
        "        train_dataset = load_dataset(\n",
        "            args.dataset_name, split=\"train\", cache_dir=model_args.cache_dir\n",
        "        )\n",
        "\n",
        "    if training_args.do_train:\n",
        "        val_dataset = load_dataset(\n",
        "            args.dataset_name, split=\"validation\", cache_dir=model_args.cache_dir\n",
        "        )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "        do_lower_case=model_args.do_lower_case,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast_tokenizer,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "\n",
        "    def preprocess_w_tokenizer(examples):\n",
        "      return preprocess_function(examples, tokenizer)\n",
        "\n",
        "    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
        "        train_dataset = train_dataset.select(range(args.train_size))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_w_tokenizer,\n",
        "            batched=True,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    # Log a few random samples from the training set:\n",
        "    for index in random.sample(range(len(train_dataset)), 3):\n",
        "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    if args.validate_iter or args.validate_layer:\n",
        "      val_arguments = get_val_arguments(args, val_dataset, model, tokenizer)\n",
        "\n",
        "    data_collator = default_data_collator\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # prune the heads of each layer\n",
        "    train_dl = trainer.get_train_dataloader()\n",
        "\n",
        "    training_dict = {\n",
        "        'args': training_args,\n",
        "        'train_dataset': train_dataset,\n",
        "        'tokenizer': tokenizer,\n",
        "        'data_collator': data_collator,\n",
        "    }\n",
        "\n",
        "    if not args.iterative:\n",
        "        ratios = [args.total_prune_ratio for x in range(6)]\n",
        "        prune_distilbert(args, model, train_dl, args.num_batches, args.iters, ratios,\n",
        "                training_dict, val_arguments, per_layer_epochs=args.epochs_per_layer, final_epochs=args.final_epochs, prune_type=args.prune_type)\n",
        "    else:\n",
        "        ratio_to_elim = 1.0 - args.total_prune_ratio\n",
        "        prune_iters = int(ratio_to_elim / args.ratio_per_iter)\n",
        "        print(f'\\nRunning {prune_iters} pruning iterations\\n')\n",
        "        for pt in range(prune_iters):\n",
        "            curr_ratio = 1.0 - (args.ratio_per_iter * (pt + 1))\n",
        "            prev_ratio = float(int((1.0 - args.ratio_per_iter * pt)*12.0) / 12.0)\n",
        "            new_ratio = curr_ratio / prev_ratio\n",
        "            ratios = [new_ratio for x in range(6)]\n",
        "            prune_distilbert(args, model, train_dl, args.num_batches, args.iters, ratios,\n",
        "                    training_dict, val_arguments, per_layer_epochs=args.epochs_per_layer, final_epochs=args.final_epochs, prune_type=args.prune_type)"
      ],
      "metadata": {
        "id": "i2RxlOvLyDLx"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run"
      ],
      "metadata": {
        "id": "y53kjzW1AKiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'output_dir': '/content',\n",
        "\n",
        "    'tokenizer_name': 'distilbert-base-uncased',\n",
        "    'model_name': 'distilbert-base-uncased-distilled-squad',\n",
        "    'dataset_name': 'squad',\n",
        "\n",
        "    'train_size': 15000,\n",
        "    'val_size': 25,\n",
        "\n",
        "    'prune_type': 'ispasp',\n",
        "\n",
        "    'final_epochs': 1,\n",
        "    'epochs_per_layer': 1,\n",
        "    'num_batches': 10,\n",
        "    'iters': 5,\n",
        "    'total_prune_ratio': 0.3,\n",
        "    'iterative': False,\n",
        "    'ratio_per_iter': 0.1,\n",
        "\n",
        "    'validate_iter': False,\n",
        "    'validate_layer': True,\n",
        "    'iter_verbose': False,\n",
        "    'verbose': False,\n",
        "\n",
        "    'dense_update': False,\n",
        "    'maintain_Q': False,\n",
        "    'eta': 5e-5,\n",
        "    'debias_iters': 5,\n",
        "    'debias_dense': False,\n",
        "}\n",
        "d_args = dotdict(args)"
      ],
      "metadata": {
        "id": "KxUd_0oE5Nym"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(d_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906,
          "referenced_widgets": [
            "093a267d82a148aa88cab8678ef40c7f",
            "cb0a98a8ecab490ea38ddeb9e1a56c9c",
            "c2004c50cc1c49528da19a93aa719e6f",
            "0a0ac276c31c46cdb3e253e384a0dd2f",
            "890fc16734c04e28ba16a22aabdb56d1",
            "f7750926384a4e4c8136fd960c162ed1",
            "38a5e2749a9845cbbadb5b604a5159d6",
            "1bbc79c16c534c0cbce991b53fe2f5a8",
            "bbdc1a978e5d47ae9074e31496edf375",
            "f4e34dd4fa0b412ca326f5e89c986d6f",
            "d9d15efaeae44145b9a8e1558522bb31"
          ]
        },
        "id": "hJeYce1H3Sa-",
        "outputId": "bf4a716e-ff07-4ebe-8727-5a47230552b0"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running tokenizer on train dataset:   0%|          | 0/15000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "093a267d82a148aa88cab8678ef40c7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Post-pruning: S = {0, 5, 7}\n",
            "Done pruning layer 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1875/1875 12:37, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.548900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.419700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.266200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =  1749191GF\n",
            "  train_loss               =     1.3813\n",
            "  train_runtime            = 0:12:37.93\n",
            "  train_samples            =      15000\n",
            "  train_samples_per_second =     19.791\n",
            "  train_steps_per_second   =      2.474\n",
            "after finetuning: {'exact_match': 28.0, 'f1': 39.733333333333334}\n",
            "Post-pruning: S = {8, 11, 5}\n",
            "Done pruning layer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  22/1875 00:07 < 11:23, 2.71 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-245979c4dbc9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-104-713de6572a90>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mratios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_prune_ratio\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         prune_distilbert(args, model, train_dl, args.num_batches, args.iters, ratios,\n\u001b[0m\u001b[1;32m    105\u001b[0m                 training_dict, val_arguments, per_layer_epochs=args.epochs_per_layer, final_epochs=args.final_epochs, prune_type=args.prune_type)\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-7ca30a677ba8>\u001b[0m in \u001b[0;36mprune_distilbert\u001b[0;34m(args, model, dl, num_batches, num_iter, ratios, training_dict, val_arguments, per_layer_epochs, final_epochs, prune_type)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'prune_layer_{i}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             model = train_and_eval(args, model, training_dict, val_arguments, per_layer_epochs,\n\u001b[0m\u001b[1;32m     41\u001b[0m                     save_path=output_dir, save_model=False, lr=5e-5, lr_decay=True, device=device)\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-965ef4bb1b33>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(args, model, train_dict, val_arguments, epochs, save_path, save_model, lr, lr_decay, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmax_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2665\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
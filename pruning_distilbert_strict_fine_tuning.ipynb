{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tNXnlzDYycri",
        "U-l0PRfFyenw",
        "bkTKpTdEQPaF",
        "VdryzDILQYua",
        "8EN5SnK6QiTb",
        "K0mGPnklQ6RB",
        "Mzzs5h5eRbBZ",
        "1il8obD1Ru_C",
        "iIF7YP0Hyni3",
        "0YrHHG_XAXOB",
        "5uzqg4OgAbEY",
        "TegeTJT-AYkN",
        "Bj7kwd_xylNW",
        "qI7fIncYepgW",
        "OStkavXOypi-",
        "XbdADMgGytn6",
        "E3VcmNnJ3Ce7",
        "2WKY_r5_yic0"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "tNXnlzDYycri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate"
      ],
      "metadata": {
        "id": "Y-oHq9Kby0PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "LJh-pxJVy1l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "import math\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from copy import deepcopy\n",
        "import time\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    DistilBertForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions\n",
        ")\n",
        "from transformers.modeling_utils import apply_chunking_to_forward\n",
        "from transformers.trainer_utils import SchedulerType\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version\n",
        "from transformers.utils.versions import require_version\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "EXO_WWgIxjcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_min_version(\"4.16.0.dev0\")\n",
        "\n",
        "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "puCM_-l8xmM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training argument classes"
      ],
      "metadata": {
        "id": "U-l0PRfFyenw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    Using `HfArgumentParser` we can turn this class\n",
        "    into argparse arguments to be able to specify them on\n",
        "    the command line.\n",
        "    \"\"\"\n",
        "\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
        "    )\n",
        "    pad_to_max_length: bool = field(\n",
        "        default=True,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
        "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n",
        "    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})"
      ],
      "metadata": {
        "id": "QE-6KkbLxpc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    language: str = field(\n",
        "        default=None, metadata={\"help\": \"Evaluation language. Also train language if `train_language` is set to None.\"}\n",
        "    )\n",
        "    train_language: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Train language if it is different from the evaluation language.\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    do_lower_case: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
        "            \"with private models).\"\n",
        "        },\n",
        "    )"
      ],
      "metadata": {
        "id": "hyvzGLGXxtQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkTKpTdEQPaF"
      },
      "source": [
        "# pruning algorithm steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdryzDILQYua"
      },
      "source": [
        "### step 1: compute gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH_MvkuUQTjJ"
      },
      "outputs": [],
      "source": [
        "def compute_grads(args, dense_out_lin, pruned_out_lin, H, weight_indexer, device='cpu'):\n",
        "  dense_out_lin = dense_out_lin.to(device)\n",
        "  U = dense_out_lin(H)\n",
        "\n",
        "  if weight_indexer is not None:\n",
        "    with torch.no_grad():\n",
        "      U_pruned = pruned_out_lin(H[:, :, weight_indexer])\n",
        "    loss = residual_objective(U - U_pruned.detach())\n",
        "  else:\n",
        "    loss = residual_objective(U)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  dW = dense_out_lin.weight.grad\n",
        "\n",
        "  # maybe instead of using attn grad, should aggregate parameter grads? either for weights only or for all\n",
        "  pruned_out_lin.zero_grad()\n",
        "  dense_out_lin.zero_grad()\n",
        "  H.grad = None\n",
        "  U.grad = None\n",
        "\n",
        "  return dW, loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EN5SnK6QiTb"
      },
      "source": [
        "### step 2: find best s columns of grad outside S and merge with S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEWVnQeLQhkD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def find_and_merge(dW, S, n_heads_og, dim_per_head, n_heads_to_keep, device='cpu'):\n",
        "  dW_by_head = dW.view(dW.shape[0], n_heads_og, dim_per_head).to(device)\n",
        "  importance = torch.norm(dW_by_head, p=2, dim=(0, 2))\n",
        "\n",
        "  for index in S:\n",
        "    importance[index] = 0\n",
        "  imp_top_idxs = torch.argsort(importance, descending=True)[:n_heads_to_keep]\n",
        "  imp_top_idxs = set(imp_top_idxs.tolist())\n",
        "\n",
        "  D = S.union(imp_top_idxs)\n",
        "\n",
        "  return D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0mGPnklQ6RB"
      },
      "source": [
        "### step 3: update parameters by gradient descent focused on D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWNeUbvIQ5zD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def update_step(args, pruned_out_lin, dense_out_lin, Q, dW, D, S_indexer, n_heads, attention_head_size, H, device=None):\n",
        "  D_not = set(range(n_heads)).difference(D)\n",
        "  not_D_indexer = get_weight_indexer(D_not, attention_head_size)\n",
        "  dW_at_D_t = deepcopy(dW)\n",
        "  if not args.dense_update:\n",
        "    dW_at_D_t[:, not_D_indexer] = 0\n",
        "\n",
        "  if not args.maintain_Q:\n",
        "    Q = torch.zeros_like(dW)\n",
        "    if S_indexer is not None:\n",
        "      Q[:, S_indexer] = pruned_out_lin.weight.data\n",
        "    else:\n",
        "      Q = deepcopy(dense_out_lin.weight.data)\n",
        "\n",
        "  Q_by_head = Q.view(Q.shape[0], n_heads, attention_head_size).to(device)\n",
        "  Q_imp = torch.norm(Q_by_head, p=2, dim=(0, 2)) # get the importance of each head\n",
        "  # print(f'before update Q_imp: {Q_imp}')\n",
        "\n",
        "  if args.eta == 'adaptive':\n",
        "     eta = torch.norm(dW_at_D_t) ** 2 / torch.norm(H @ dW_at_D_t.T) ** 2\n",
        "  else:\n",
        "    eta = args.eta\n",
        "\n",
        "  # print(f'update grad * eta: {eta * dW_at_D_t}')\n",
        "  Q = Q - eta * dW_at_D_t\n",
        "\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzzs5h5eRbBZ"
      },
      "source": [
        "### step 4: truncate Q to be s-sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWLNhMImRgwn"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def truncate(Q, pruned_out_lin, n_heads_og, dim_per_head, n_heads_to_keep, device='cpu'):\n",
        "  Q_by_head = Q.view(Q.shape[0], n_heads_og, dim_per_head).to(device)\n",
        "\n",
        "  Q_imp = torch.norm(Q_by_head, p=2, dim=(0, 2)) # get the importance of each head\n",
        "  # print(f'Q_imp: {Q_imp}')\n",
        "  imp_top_idxs = torch.argsort(Q_imp, descending=True)[:n_heads_to_keep]\n",
        "  S = set(imp_top_idxs.cpu().tolist())\n",
        "\n",
        "  S_indexer = get_weight_indexer(S, dim_per_head)\n",
        "  pruned_out_lin.weight.data = Q[:, S_indexer]\n",
        "\n",
        "  return pruned_out_lin, S, S_indexer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1il8obD1Ru_C"
      },
      "source": [
        "### step 5: debias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvcRPgoWRxNg"
      },
      "outputs": [],
      "source": [
        "def debias(args, pruned_out_lin, dense_out_lin, Q, H, S_indexer, iters, device='cpu'):\n",
        "  H_pruned = H[:, :, S_indexer]\n",
        "  for i in range(iters):\n",
        "    # print(f'debias iter {i}')\n",
        "    pruned_out_lin.zero_grad()\n",
        "    dense_out_lin.zero_grad()\n",
        "\n",
        "    if args.debias_dense:\n",
        "      U_dense = dense_out_lin(H)\n",
        "      with torch.no_grad():\n",
        "        U_pruned = pruned_out_lin(H_pruned)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        U_dense = dense_out_lin(H)\n",
        "      U_pruned = pruned_out_lin(H_pruned)\n",
        "\n",
        "    if args.debias_dense:\n",
        "      loss = residual_objective(U_dense - U_pruned.detach())\n",
        "    else:\n",
        "      loss = residual_objective(U_dense.detach() - U_pruned)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      if args.debias_dense:\n",
        "        dW = dense_out_lin.weight.grad\n",
        "        if args.eta == 'adaptive':\n",
        "          eta = torch.norm(dW) ** 2 / torch.norm(H @ dW.T) ** 2\n",
        "        else:\n",
        "          eta = args.eta\n",
        "\n",
        "        Q = Q - eta * dW\n",
        "        pruned_out_lin.weight.data = Q[:, S_indexer]\n",
        "\n",
        "      else:\n",
        "        dW_pruned = pruned_out_lin.weight.grad\n",
        "\n",
        "        if args.eta == 'adaptive':\n",
        "          eta = torch.norm(dW_pruned) ** 2 / torch.norm(H_pruned @ dW_pruned.T) ** 2\n",
        "        else:\n",
        "          eta = args.eta\n",
        "\n",
        "        # print(f'debias grad: {dW_pruned}')\n",
        "        # print(f'debias weight: {pruned_out_lin.weight.data}')\n",
        "\n",
        "        pruned_out_lin.weight.data = pruned_out_lin.weight.data - eta * dW_pruned\n",
        "        Q[:, S_indexer] = pruned_out_lin.weight.data\n",
        "\n",
        "  return pruned_out_lin, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "iIF7YP0Hyni3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## general"
      ],
      "metadata": {
        "id": "0YrHHG_XAXOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dotdict:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            if isinstance(value, dict):\n",
        "                setattr(self, key, dotdict(value))\n",
        "            else:\n",
        "                setattr(self, key, value)"
      ],
      "metadata": {
        "id": "z45lWKWc5bIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrn(ckpt_path, model):\n",
        "    print(f'\\n\\nLoading model from: {ckpt_path}\\n\\n')\n",
        "    #CONFIG_NAME = 'config.json'\n",
        "    WEIGHTS_NAME = 'pytorch_model.bin'\n",
        "    #cfg_path = os.path.join(ckpt_path, CONFIG_NAME)\n",
        "    weight_path = os.path.join(ckpt_path, WEIGHTS_NAME)\n",
        "    assert os.path.exists(weight_path)\n",
        "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state_dict)\n",
        "    del state_dict\n",
        "    return model"
      ],
      "metadata": {
        "id": "gC7CduCAyFDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transformer_arguments(model, data, device='cpu'):\n",
        "  model = model.to(device)\n",
        "  input_ids = data['input_ids'].to(device)\n",
        "  with torch.no_grad():\n",
        "    embeddings = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "  input_shape = input_ids.size()\n",
        "\n",
        "  attention_mask = data['attention_mask']\n",
        "  if attention_mask is None:\n",
        "      attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
        "\n",
        "  return embeddings, attention_mask"
      ],
      "metadata": {
        "id": "uNxYtU8b1OvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pruning"
      ],
      "metadata": {
        "id": "5uzqg4OgAbEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate heads\n",
        "def separate_heads(x, bs, n_heads, dim_per_head):\n",
        "  return x.view(bs, -1, n_heads, dim_per_head).transpose(1, 2)"
      ],
      "metadata": {
        "id": "7Uwxumfi2mdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight_indexer(head_list, head_dim):\n",
        "  indices = []\n",
        "  for i in head_list:\n",
        "      indices.extend(list(range(head_dim*i, head_dim*(i + 1))))\n",
        "  indices = sorted(indices)\n",
        "  return torch.LongTensor(indices)"
      ],
      "metadata": {
        "id": "Gq4wCvvn2lFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_objective(mat):\n",
        "    return 0.5 * torch.norm(mat) # using torch.sum requires small stepsize (order of 1e-5)"
      ],
      "metadata": {
        "id": "sAU4XOtW2kNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(model, data):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    iids = data['input_ids'].to(device)\n",
        "    ttids = data['token_type_ids'].to(device)\n",
        "    att_mask = data['attention_mask'].to(device)\n",
        "    ext_att_mask = model.bert.get_extended_attention_mask(att_mask, iids.size(), device)\n",
        "    #head_mask = model.bert.get_head_mask(None, model.bert.config.num_hidden_layers)\n",
        "    embedding_output = model.bert.embeddings(input_ids=iids, token_type_ids=ttids)\n",
        "    return embedding_output, ext_att_mask"
      ],
      "metadata": {
        "id": "RbkusaY4zEB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "TegeTJT-AYkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples, tokenizer):\n",
        "    # Tokenize the texts\n",
        "    inputs = tokenizer(\n",
        "      examples['question'],\n",
        "      examples[\"context\"],\n",
        "      max_length=512,\n",
        "      truncation=\"only_second\",\n",
        "      stride=128,\n",
        "      padding=\"max_length\",\n",
        "      return_tensors='pt',\n",
        "      return_offsets_mapping=True,\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "wwfZagaK43tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# finetune"
      ],
      "metadata": {
        "id": "Bj7kwd_xylNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(args, model, train_dict, val_arguments, epochs, save_path=None, save_model=False, lr=5e-5, lr_decay=True, device='cpu'):\n",
        "    model.eval()\n",
        "    val_arguments['model'] = model\n",
        "    metrics = validate(args, **val_arguments, device=device)\n",
        "    print(f'before finetuning: {metrics}')\n",
        "\n",
        "    train_dict['args'].learning_rate = lr\n",
        "    if lr_decay:\n",
        "        train_dict['args'].lr_scheduler_type = SchedulerType.LINEAR\n",
        "    else:\n",
        "        train_dict['args'].lr_scheduler_type = SchedulerType.CONSTANT\n",
        "    train_dict['args'].num_train_epochs = float(epochs)\n",
        "    trainer = Trainer(model=model, **train_dict)\n",
        "    model.train()\n",
        "    train_result = trainer.train(resume_from_checkpoint=None)\n",
        "    metrics = train_result.metrics\n",
        "    max_train_samples = len(train_dict['train_dataset'])\n",
        "    metrics[\"train_samples\"] = max_train_samples\n",
        "    if save_path is not None:\n",
        "        trainer.output_dir = save_path\n",
        "        trainer.run_name = save_path\n",
        "        if save_model:\n",
        "            trainer.save_model()\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "\n",
        "    model.eval()\n",
        "    val_arguments['model'] = model\n",
        "    metrics = validate(args, **val_arguments, device=device)\n",
        "    print(f'after finetuning: {metrics}')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "WJ2164FJyGnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validate"
      ],
      "metadata": {
        "id": "qI7fIncYepgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(args, model, ids, inputs, answers, metric, tokenizer, device='cpu'):\n",
        "  model = model.to(device)\n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad():\n",
        "    start_sp = time.time()\n",
        "    outputs_sp = model(**inputs)\n",
        "    end_sp = time.time()\n",
        "    if args.verbose:\n",
        "      print(f'done predicting using sparse model. time elapsed = {end_sp - start_sp}s')\n",
        "\n",
        "  num_examples = len(ids)\n",
        "  preds = []\n",
        "  refs = []\n",
        "  for i in range(num_examples):\n",
        "    answer_start_index_sp = torch.argmax(outputs_sp.start_logits[i])\n",
        "    answer_end_index_sp = torch.argmax(outputs_sp.end_logits[i])\n",
        "    predict_answer_tokens_sp = inputs.input_ids[i, answer_start_index_sp : answer_end_index_sp + 1]\n",
        "    pred_sp = tokenizer.decode(predict_answer_tokens_sp)\n",
        "\n",
        "    pred = {'id': ids[i], 'prediction_text': pred_sp}\n",
        "    preds.append(pred)\n",
        "    ref = {'answers': answers[i], 'id': ids[i]}\n",
        "    refs.append(ref)\n",
        "\n",
        "  results = metric.compute(predictions=preds, references=refs)\n",
        "  return results"
      ],
      "metadata": {
        "id": "JBp0XKExQ7Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_val_arguments(args, val_ds, model, tokenizer):\n",
        "      val_arguments = {}\n",
        "      start_idx = torch.randint(low=0, high=len(val_ds['id']) - args.val_size, size=(1,)).item()\n",
        "\n",
        "      val_ids = val_ds['id'][start_idx : start_idx + args.val_size]\n",
        "      val_questions =  val_ds['question'][start_idx : start_idx + args.val_size]\n",
        "      val_texts =  val_ds['context'][start_idx : start_idx + args.val_size]\n",
        "      val_answers =  val_ds['answers'][start_idx : start_idx + args.val_size]\n",
        "\n",
        "      val_inputs = tokenizer(\n",
        "            val_questions,\n",
        "            val_texts,\n",
        "            max_length=512,\n",
        "            truncation=\"only_second\",\n",
        "            stride=128,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors='pt'\n",
        "      )\n",
        "\n",
        "      val_arguments['ids'] = val_ids\n",
        "      val_arguments['inputs'] = val_inputs\n",
        "      val_arguments['answers'] = val_answers\n",
        "      val_arguments['model'] = deepcopy(model)\n",
        "      val_arguments['metric'] = evaluate.load(args.dataset_name)\n",
        "      val_arguments['tokenizer'] = tokenizer\n",
        "\n",
        "      return val_arguments"
      ],
      "metadata": {
        "id": "A-77DUb7_McO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# comparisons"
      ],
      "metadata": {
        "id": "OStkavXOypi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_layer_random_prune(blay, prune_ratio=0.5):\n",
        "    satt = blay.attention.self\n",
        "    num_heads_to_prune = int((1 - prune_ratio)*satt.num_attention_heads)\n",
        "    heads_to_prune = list(range(satt.num_attention_heads))\n",
        "    random.shuffle(heads_to_prune)\n",
        "    heads_to_prune = heads_to_prune[:num_heads_to_prune]\n",
        "    blay.attention.prune_heads(heads_to_prune)\n",
        "    return blay"
      ],
      "metadata": {
        "id": "WwEvzk73yUHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_global_masking(model, dl, num_batches=10, ratio=0.5):\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    num_layers = len(model.bert.encoder.layer)\n",
        "    num_heads = model.bert.encoder.layer[0].attention.self.num_attention_heads\n",
        "    final_num_heads = int(ratio * num_layers * num_heads)\n",
        "    mask_tensor = torch.ones(int(num_heads*num_layers))\n",
        "    mask_tensor = mask_tensor.to(device)\n",
        "    mask_tensor.requires_grad = True\n",
        "    for b in range(num_batches):\n",
        "        data_in = next(iter(dl))\n",
        "        labels = data_in['labels']\n",
        "        labels = labels.to(device)\n",
        "        hidden_rep, ext_att_mask = get_bert_embeddings(model, data_in)\n",
        "\n",
        "        # pass data thru each layer of the model\n",
        "        for i in range(num_layers):\n",
        "            blay = model.bert.encoder.layer[i]\n",
        "            satt = blay.attention.self\n",
        "            sout = blay.attention.output\n",
        "\n",
        "            query_out = satt.transpose_for_scores(satt.query(hidden_rep))\n",
        "            key_out = satt.transpose_for_scores(satt.key(hidden_rep))\n",
        "            value_out = satt.transpose_for_scores(satt.value(hidden_rep))\n",
        "\n",
        "            att_sc = torch.matmul(query_out, key_out.transpose(-1, -2)) # [32, 12, 128, 128], has all heads separated\n",
        "            att_sc = att_sc / math.sqrt(satt.attention_head_size)\n",
        "            att_sc += ext_att_mask # contains a bunch of negative infinities to remove stuff in softmax computation\n",
        "            att_prob = torch.nn.functional.softmax(att_sc, dim=-1)\n",
        "\n",
        "            ctxt = torch.matmul(att_prob, value_out) # [32, 12, 128, 64] still separated btwn heads\n",
        "            ctxt = ctxt.permute(0, 2, 1, 3).contiguous() # [32, 128, 12, 64]\n",
        "            sense_mask = mask_tensor[num_heads*i: num_heads*(i + 1)]\n",
        "\n",
        "            ctxt = ctxt * sense_mask[None, None, :, None] # add mask into the forward pass\n",
        "\n",
        "            new_shape = ctxt.size()[:-2] + (satt.all_head_size,)\n",
        "            ctxt = ctxt.view(*new_shape) # [32, 128, 768], THIS IS THE OUTPUT OF SELF ATTN\n",
        "\n",
        "            out = sout.dense(ctxt)\n",
        "            out = sout.LayerNorm(out + hidden_rep)\n",
        "\n",
        "            hidden_rep = apply_chunking_to_forward(blay.feed_forward_chunk,\n",
        "                    blay.chunk_size_feed_forward, blay.seq_len_dim, out)\n",
        "\n",
        "        output = BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=hidden_rep,\n",
        "                past_key_values=None, hidden_states=None, attentions=None,\n",
        "                cross_attentions=None)\n",
        "        pooled_output = model.bert.pooler(output[0])\n",
        "        pooled_output_wrapper = BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "                last_hidden_state=output[0], pooler_output=pooled_output,\n",
        "                past_key_values=None, hidden_states=None, attentions=None,\n",
        "                cross_attentions=None)\n",
        "        pool_out = pooled_output_wrapper[1]\n",
        "        logits = model.classifier(pool_out)\n",
        "        loss = loss_fn(logits.view(-1, model.num_labels), labels.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "    agg_grad = mask_tensor.grad.detach().cpu()\n",
        "    with torch.no_grad():\n",
        "        agg_grad = torch.abs(agg_grad)\n",
        "    imp_idxs = set(torch.argsort(agg_grad, descending=True)[:final_num_heads].cpu().tolist())\n",
        "    for l in range(num_layers):\n",
        "        heads_to_prune = []\n",
        "        for hi in range(num_heads):\n",
        "             curr_idx = num_heads * l + hi\n",
        "             if not curr_idx in imp_idxs:\n",
        "                 heads_to_prune.append(hi)\n",
        "        assert len(heads_to_prune) < num_heads\n",
        "        model.bert.encoder.layer[l].attention.prune_heads(heads_to_prune)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "IGDNa_9gyVQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prune model"
      ],
      "metadata": {
        "id": "XbdADMgGytn6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFsFSVBAxdKJ"
      },
      "outputs": [],
      "source": [
        "def prune_distilbert(args, model, dl, num_batches, num_iter, ratios,\n",
        "        training_dict, val_arguments, per_layer_epochs, final_epochs, prune_type='ispasp'):\n",
        "\n",
        "    # performs i-SpaSP pruning on the distilBERT model for squad\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # get the output dir\n",
        "    save_base = training_dict['args'].output_dir\n",
        "\n",
        "    transformer = model.distilbert.transformer\n",
        "    layers = transformer.layer\n",
        "\n",
        "    metrics = validate(args, **val_arguments, device=device)\n",
        "    print(f'Pre-pruning metrics = {metrics}')\n",
        "    # start passing thru each layer of encoder\n",
        "    if prune_type in ['random', 'ispasp']:\n",
        "        for i, rat in enumerate(ratios):\n",
        "            if i < args.min_layer or i > args.max_layer:\n",
        "              continue\n",
        "            if prune_type == 'ispasp':\n",
        "                data_list = []\n",
        "                mask_list = []\n",
        "                for b in range(num_batches):\n",
        "                    data_in = next(iter(dl))\n",
        "                    with torch.no_grad():\n",
        "                        model.eval()\n",
        "                        hidden_rep, ext_att_mask = get_transformer_arguments(model, data_in, device)\n",
        "                        # hidden_rep, ext_att_mask = get_bert_embeddings(model, data_in)\n",
        "                        for j in range(i):\n",
        "                            hidden_rep = layers[j](hidden_rep, ext_att_mask)[0]\n",
        "                            #hidden_rep = bert_layer_forward(model.bert.encoder.layer[j],\n",
        "                            #        hidden_rep, ext_att_mask)\n",
        "                        data_list.append(hidden_rep.detach().cpu())\n",
        "                        mask_list.append(ext_att_mask.detach().cpu())\n",
        "                layers[i].attention = prune_attn_layer(args, layers[i].attention, data_list, mask_list, val_arguments, prune_ratio=rat, layer_id=i, device=device)\n",
        "\n",
        "            elif prune_type == 'random':\n",
        "                layers[i] = bert_layer_random_prune(\n",
        "                        layers[i], prune_ratio=rat)\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "            output_dir = os.path.join(save_base, f'prune_layer_{i}/')\n",
        "            model = train_and_eval(args, model, training_dict, val_arguments, per_layer_epochs,\n",
        "                    save_path=output_dir, save_model=False, lr=5e-5, lr_decay=True, device=device)\n",
        "\n",
        "        model = train_and_eval(args, model, training_dict, val_arguments, final_epochs,\n",
        "                save_path=save_base, save_model=True, lr=5e-5, lr_decay=True, device=device)\n",
        "\n",
        "    elif prune_type == 'masking':\n",
        "        model = prune_global_masking(model, dl, num_batches=num_batches, ratio=ratios[0])\n",
        "        model = train_and_eval(args, model, training_dict, val_arguments, final_epochs,\n",
        "                save_path=save_base, save_model=True, lr=5e-5, lr_decay=True, device=device)\n",
        "    else:\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prune layer"
      ],
      "metadata": {
        "id": "E3VcmNnJ3Ce7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_attn_layer(args, layer, hidden_states_list, mask_list, val_arguments, prune_ratio=0.5, layer_id=None, device='cpu'):\n",
        "  if layer_id is not None:\n",
        "    print(f'\\nPruning layer {layer_id}')\n",
        "\n",
        "  n_heads_to_keep = int(layer.n_heads * prune_ratio)\n",
        "\n",
        "  def shape(x):\n",
        "    \"\"\"separate heads\"\"\"\n",
        "    return x.view(x.shape[0], -1, layer.n_heads, layer.attention_head_size).transpose(1, 2)\n",
        "\n",
        "  def unshape(x):\n",
        "      \"\"\"group heads\"\"\"\n",
        "      return x.transpose(1, 2).contiguous().view(x.shape[0], -1, layer.n_heads * layer.attention_head_size)\n",
        "\n",
        "  dense_out_lin = layer.out_lin.to(device)\n",
        "  pruned_out_lin = nn.Linear(in_features = layer.attention_head_size * n_heads_to_keep,\n",
        "                             out_features = layer.dim,\n",
        "                             bias = True).to(device)\n",
        "  Q = deepcopy(dense_out_lin.weight.data)\n",
        "\n",
        "  S = set([]) # heads to keep\n",
        "  S_indexer = None\n",
        "\n",
        "  layer = layer.to(device)\n",
        "\n",
        "  hidden_states = torch.cat(hidden_states_list, dim=0)\n",
        "  attn_mask = torch.cat(mask_list, dim=0)\n",
        "  with torch.no_grad():\n",
        "      attn_mask = attn_mask.to(device)\n",
        "      hidden_states = hidden_states.to(device)\n",
        "      query, key, value = hidden_states, hidden_states, hidden_states\n",
        "\n",
        "      _, weights = layer(query, key, value, attn_mask, head_mask=None, output_attentions=True)\n",
        "\n",
        "      v = shape(layer.v_lin(value))\n",
        "      context = torch.matmul(weights, v)\n",
        "      H = unshape(context)\n",
        "\n",
        "  for t in range(args.iters):\n",
        "    if args.validate_iter:\n",
        "      val_arguments['model'].distilbert.transformer.layer[layer_id].attention.out_lin = pruned_out_lin # might not work with dimensions\n",
        "      acc = validate(args, **val_arguments, device=device)\n",
        "      pruned_out_lin = pruned_out_lin.to(device)\n",
        "\n",
        "    dW, loss = compute_grads(args, dense_out_lin, pruned_out_lin, H, S_indexer, device=device)\n",
        "\n",
        "    if args.validate_iter:\n",
        "      print(f'Iteration {t}: S = {S} | Loss = {loss} | Exact Match = {acc[\"exact_match\"]} | F1 = {acc[\"f1\"]}')\n",
        "    elif args.iter_verbose:\n",
        "      print(f'Iteration {t}: S = {S} | Loss = {loss}')\n",
        "\n",
        "    D = find_and_merge(dW, S, layer.n_heads, layer.attention_head_size, n_heads_to_keep, device=device)\n",
        "    Q = update_step(args, pruned_out_lin, dense_out_lin, Q, dW, D, S_indexer, layer.n_heads, layer.attention_head_size, H, device=None)\n",
        "    pruned_out_lin, S, S_indexer = truncate(Q, pruned_out_lin, layer.n_heads, layer.attention_head_size, n_heads_to_keep, device=device)\n",
        "    pruned_out_lin, Q = debias(args, pruned_out_lin, dense_out_lin, Q, H, S_indexer, args.debias_iters, device=device)\n",
        "\n",
        "  print(f'Post-pruning: S = {S}')\n",
        "  all_heads = set(range(layer.n_heads))\n",
        "\n",
        "  layer.prune_heads(all_heads.difference(S))\n",
        "  layer.out_lin = pruned_out_lin\n",
        "\n",
        "  if layer_id is not None:\n",
        "    print(f'Done pruning layer {layer_id}')\n",
        "\n",
        "  return layer"
      ],
      "metadata": {
        "id": "tT-9ixBXzs_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "2WKY_r5_yic0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    model_args = ModelArguments(tokenizer_name=args.tokenizer_name,\n",
        "                                model_name_or_path=args.model_name)\n",
        "\n",
        "    data_args = DataTrainingArguments()\n",
        "\n",
        "    training_args = TrainingArguments(output_dir=args.output_dir,\n",
        "                                      do_train=True,\n",
        "                                      do_eval=False)\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "\n",
        "    log_level = training_args.get_process_log_level()\n",
        "    logger.setLevel(log_level)\n",
        "    datasets.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.enable_default_handler()\n",
        "    transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "    # Log on each process the small summary:\n",
        "    logger.warning(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu} \"\n",
        "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Detecting last checkpoint.\n",
        "    last_checkpoint = None\n",
        "\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    if training_args.do_train:\n",
        "        train_dataset = load_dataset(\n",
        "            args.dataset_name, split=\"train\", cache_dir=model_args.cache_dir\n",
        "        )\n",
        "\n",
        "    if training_args.do_train:\n",
        "        val_dataset = load_dataset(\n",
        "            args.dataset_name, split=\"validation\", cache_dir=model_args.cache_dir\n",
        "        )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "        # do_lower_case=model_args.do_lower_case,\n",
        "        # cache_dir=model_args.cache_dir,\n",
        "        # use_fast=model_args.use_fast_tokenizer,\n",
        "        # revision=model_args.model_revision,\n",
        "        # use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "\n",
        "    def preprocess_w_tokenizer(examples):\n",
        "      return preprocess_function(examples, tokenizer)\n",
        "\n",
        "    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
        "        train_dataset = train_dataset.select(range(args.train_size))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_w_tokenizer,\n",
        "            batched=True,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    # Log a few random samples from the training set:\n",
        "    for index in random.sample(range(len(train_dataset)), 3):\n",
        "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    if args.validate_iter or args.validate_layer:\n",
        "      val_arguments = get_val_arguments(args, val_dataset, model, tokenizer)\n",
        "\n",
        "    data_collator = default_data_collator\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # prune the heads of each layer\n",
        "    train_dl = trainer.get_train_dataloader()\n",
        "\n",
        "    training_dict = {\n",
        "        'args': training_args,\n",
        "        'train_dataset': train_dataset,\n",
        "        'tokenizer': tokenizer,\n",
        "        'data_collator': data_collator,\n",
        "    }\n",
        "\n",
        "    if not args.iterative:\n",
        "        ratios = [args.total_prune_ratio for x in range(6)]\n",
        "        prune_distilbert(args, model, train_dl, args.num_batches, args.iters, ratios,\n",
        "                training_dict, val_arguments, per_layer_epochs=args.epochs_per_layer, final_epochs=args.final_epochs, prune_type=args.prune_type)\n",
        "    else:\n",
        "        ratio_to_elim = 1.0 - args.total_prune_ratio\n",
        "        prune_iters = int(ratio_to_elim / args.ratio_per_iter)\n",
        "        print(f'\\nRunning {prune_iters} pruning iterations\\n')\n",
        "        for pt in range(prune_iters):\n",
        "            curr_ratio = 1.0 - (args.ratio_per_iter * (pt + 1))\n",
        "            prev_ratio = float(int((1.0 - args.ratio_per_iter * pt)*12.0) / 12.0)\n",
        "            new_ratio = curr_ratio / prev_ratio\n",
        "            ratios = [new_ratio for x in range(6)]\n",
        "            prune_distilbert(args, model, train_dl, args.num_batches, args.iters, ratios,\n",
        "                    training_dict, val_arguments, per_layer_epochs=args.epochs_per_layer, final_epochs=args.final_epochs, prune_type=args.prune_type)"
      ],
      "metadata": {
        "id": "i2RxlOvLyDLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run"
      ],
      "metadata": {
        "id": "y53kjzW1AKiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'output_dir': '/content',\n",
        "\n",
        "    'tokenizer_name': 'distilbert-base-uncased-distilled-squad',\n",
        "    'model_name': 'distilbert-base-uncased-distilled-squad',\n",
        "    'dataset_name': 'squad',\n",
        "\n",
        "    'train_size': 1500,\n",
        "    'val_size': 25,\n",
        "\n",
        "    'prune_type': 'ispasp',\n",
        "\n",
        "    'final_epochs': 1,\n",
        "    'epochs_per_layer': 1,\n",
        "    'num_batches': 10,\n",
        "    'iters': 5,\n",
        "    'total_prune_ratio': 0.3,\n",
        "    'iterative': False,\n",
        "    'ratio_per_iter': 0.1,\n",
        "    'min_layer': 4,\n",
        "    'max_layer': 4,\n",
        "\n",
        "    'validate_iter': False,\n",
        "    'validate_layer': True,\n",
        "    'iter_verbose': False,\n",
        "    'verbose': False,\n",
        "\n",
        "    'dense_update': False,\n",
        "    'maintain_Q': False,\n",
        "    'eta': 5e-5,\n",
        "    'debias_iters': 5,\n",
        "    'debias_dense': False,\n",
        "}\n",
        "d_args = dotdict(args)"
      ],
      "metadata": {
        "id": "KxUd_0oE5Nym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(d_args)"
      ],
      "metadata": {
        "id": "hJeYce1H3Sa-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
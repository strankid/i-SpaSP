{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## save to drive"
      ],
      "metadata": {
        "id": "hj1sHOfZvUjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFm_bIN6vUAv",
        "outputId": "5159e77d-98af-4a2d-f28a-fa3173fb58ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/pruned_transformer30pct.pth /content/pruned_transformer30pct_all_layers.pth"
      ],
      "metadata": {
        "id": "Z2lUpeg--lGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/pruned_transformer30pct_all_layers.pth /content/gdrive/MyDrive/Research/Data/"
      ],
      "metadata": {
        "id": "Ptf7KN_JSiwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3qZ8rlt8Qpz"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMSxS7ly3Tzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8f16c1-bcb5-4442-de20-1344bef03682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6cnbDGc3XrM"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "from transformers.data.metrics import squad_metrics\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import random\n",
        "from transformers.pytorch_utils import find_pruneable_heads_and_indices\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkTKpTdEQPaF"
      },
      "source": [
        "## pruning algorithm steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdryzDILQYua"
      },
      "source": [
        "### step 1: compute gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH_MvkuUQTjJ"
      },
      "outputs": [],
      "source": [
        "def compute_grads(pruned_layer, dense_layer, H, attn_mask, head_mask, pruned_indices, device='cpu'):\n",
        "  query, key, value = H, H, H\n",
        "  U_dense = dense_layer(query, key, value, attn_mask, head_mask=head_mask, output_attentions=False)[-1]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if len(pruned_indices) > 0:\n",
        "      U_pruned = pruned_layer(query, key, value, attn_mask, head_mask=head_mask, output_attentions=False)[-1]\n",
        "\n",
        "  if len(pruned_indices) > 0:\n",
        "    loss = residual_objective(U_pruned - U_dense)\n",
        "  else:\n",
        "    loss = residual_objective(U_dense)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  importance = None\n",
        "  with torch.no_grad():\n",
        "    grad_dict = {}\n",
        "    for name, parameter in dense_layer.named_parameters():\n",
        "      grad_dict[name] = parameter.grad\n",
        "\n",
        "      if 'weight' in name:\n",
        "        if 'out' in name:\n",
        "          grad_reshaped = parameter.grad.view(dense_layer.dim, dense_layer.n_heads, dense_layer.attention_head_size)\n",
        "          imp_by_head = torch.norm(grad_reshaped, p=2, dim=(0,2))\n",
        "        else:\n",
        "          grad_reshaped = parameter.grad.view(dense_layer.n_heads, dense_layer.attention_head_size ,dense_layer.dim)\n",
        "          imp_by_head = torch.norm(grad_reshaped, p=2, dim=(1,2))\n",
        "\n",
        "        if importance is None:\n",
        "          importance = torch.zeros_like(imp_by_head)\n",
        "        importance += imp_by_head\n",
        "\n",
        "    pruned_layer.zero_grad()\n",
        "    dense_layer.zero_grad()\n",
        "\n",
        "  return importance, grad_dict, loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EN5SnK6QiTb"
      },
      "source": [
        "### step 2: find best s columns of grad outside S and merge with S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEWVnQeLQhkD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def find_and_merge(importance, S, n_heads_to_keep, device='cpu'):\n",
        "  for index in S:\n",
        "    importance[index] = 0\n",
        "  imp_top_idxs = torch.argsort(importance, descending=True)[:n_heads_to_keep]\n",
        "  imp_top_idxs = set(imp_top_idxs.tolist())\n",
        "  # print(f'top idxs of grad: {imp_top_idxs}, S: {S}')\n",
        "  D = S.union(imp_top_idxs)\n",
        "\n",
        "  return D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0mGPnklQ6RB"
      },
      "source": [
        "### step 3: update parameters by gradient descent focused on D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWNeUbvIQ5zD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def update_step(args, Q, dense_layer, grad_dict, qkv_optimizer, out_optimizer, D, device=None):\n",
        "  if not args.maintain_Q:\n",
        "    Q = deepcopy(dense_layer)\n",
        "\n",
        "  for name, Q_param in Q.named_parameters():\n",
        "    if name in grad_dict:\n",
        "      Q_param.grad = grad_dict[name]\n",
        "    else:\n",
        "      print(f'No gradient for {name}')\n",
        "\n",
        "  indexer = get_mask_indexer(D, Q.n_heads, Q.attention_head_size)\n",
        "  if args.maintain_Q:\n",
        "    qkv_optimizer.step(indexer)\n",
        "    out_optimizer.step(indexer)\n",
        "\n",
        "  else:\n",
        "    for name, Q_param in Q.named_parameters():\n",
        "      grad = Q_param.grad\n",
        "      if 'out' in name:\n",
        "        if 'weight' in name:\n",
        "          grad[:, indexer] = 0\n",
        "      else:\n",
        "        grad[indexer] = 0\n",
        "      Q_param = Q_param - args.eta * grad\n",
        "\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzzs5h5eRbBZ"
      },
      "source": [
        "### step 4: truncate Q to be s-sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWLNhMImRgwn"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def truncate(Q, pruned_layer, H, attn_mask, head_mask, n_heads_to_keep, device='cpu'): # truncate based on weights? or combine with WANDA? or truncate based on output? create args.trunc_strategy and try all of these.\n",
        "  # attn = Q(H, H, H, attn_mask, head_mask=head_mask, output_attentions=False)[-1] # can also compute some norm over the different params\n",
        "  # attn_sep = separate_heads(attn, attn.shape[0], Q.n_heads, Q.dim // Q.n_heads)\n",
        "  # attn_imp = torch.norm(attn_sep, p=2, dim=(0, 2, 3))\n",
        "\n",
        "  importance = None\n",
        "  if args.trunc_strategy == 'magnitude':\n",
        "    for name, param in Q.named_parameters():\n",
        "      if 'weight' in name:\n",
        "        if 'out' in name:\n",
        "          param_reshaped = param.view(Q.dim, Q.n_heads, Q.attention_head_size)\n",
        "          imp_by_head = torch.norm(param_reshaped, p=2, dim=(0,2))\n",
        "        else:\n",
        "          grad_reshaped = param.view(Q.n_heads, Q.attention_head_size, Q.dim)\n",
        "          imp_by_head = torch.norm(grad_reshaped, p=2, dim=(1,2))\n",
        "        if importance is None:\n",
        "          importance = torch.zeros_like(imp_by_head)\n",
        "        importance += imp_by_head\n",
        "  imp_top_idxs = torch.argsort(importance, descending=True)[:n_heads_to_keep]\n",
        "  S = set(imp_top_idxs.cpu().tolist())\n",
        "\n",
        "  pruned_layer = deepcopy(Q)\n",
        "  not_S = set(range(Q.n_heads)).difference(S)\n",
        "  pruned_layer.prune_heads(not_S)\n",
        "\n",
        "  return pruned_layer, S"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1il8obD1Ru_C"
      },
      "source": [
        "### step 5: debias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvcRPgoWRxNg"
      },
      "outputs": [],
      "source": [
        "def debias(args, pruned_layer, dense_layer, input_loader, iters, head_mask, eta=None, device='cpu'):\n",
        "  # optimizer = optim.SGD(pruned_layer.parameters())\n",
        "\n",
        "  optimizer = optim.Adam(pruned_layer.parameters(), lr=args.eta)\n",
        "  for i in range(iters):\n",
        "    attn_mask, H = next(iter(input_loader))\n",
        "\n",
        "    query = key = value = H.to(device)\n",
        "    attn_mask.to(device)\n",
        "\n",
        "    pruned_layer.zero_grad()\n",
        "    dense_layer.zero_grad()\n",
        "\n",
        "    U_pruned = pruned_layer(query, key, value, attn_mask, head_mask=head_mask, output_attentions=False)[-1]\n",
        "    with torch.no_grad():\n",
        "      U_dense = dense_layer(query, key, value, attn_mask, head_mask=head_mask, output_attentions=False)[-1]\n",
        "\n",
        "    loss = residual_objective(U_pruned - U_dense)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      optimizer.step()\n",
        "\n",
        "  return pruned_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pruning algorithm utils"
      ],
      "metadata": {
        "id": "IHk6P4I8Gxdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(args, model, ids, inputs, answers, metric, device='cpu'):\n",
        "  model = model.to(device)\n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad():\n",
        "    start_sp = time.time()\n",
        "    outputs_sp = model(**inputs)\n",
        "    end_sp = time.time()\n",
        "    if args.verbose:\n",
        "      print(f'done predicting using sparse model. time elapsed = {end_sp - start_sp}s')\n",
        "\n",
        "  model = model.cpu()\n",
        "  inputs = inputs.to('cpu')\n",
        "\n",
        "  num_examples = len(ids)\n",
        "  preds = []\n",
        "  refs = []\n",
        "  for i in range(num_examples):\n",
        "    answer_start_index_sp = torch.argmax(outputs_sp.start_logits[i])\n",
        "    answer_end_index_sp = torch.argmax(outputs_sp.end_logits[i])\n",
        "    predict_answer_tokens_sp = inputs.input_ids[i, answer_start_index_sp : answer_end_index_sp + 1]\n",
        "    pred_sp = tokenizer.decode(predict_answer_tokens_sp)\n",
        "\n",
        "    pred = {'id': ids[i], 'prediction_text': pred_sp}\n",
        "    preds.append(pred)\n",
        "    ref = {'answers': answers[i], 'id': ids[i]}\n",
        "    refs.append(ref)\n",
        "\n",
        "  results = metric.compute(predictions=preds, references=refs)\n",
        "\n",
        "  #   possible_answers = answers[i]['text']\n",
        "  #   for j in range(len(possible_answers)):\n",
        "  #     possible_answers[j] = possible_answers[j].lower()\n",
        "\n",
        "  #   correct_sp = int(answer_sp in possible_answers)\n",
        "\n",
        "  #   if args.verbose:\n",
        "  #     print('\\nTEXT:', texts[i])\n",
        "  #     print('QUESTION:', questions[i])\n",
        "  #     print('MODEL ANSWER:', answer_sp)\n",
        "  #     print('EXPECTED ANSWER:', possible_answers)\n",
        "  #     print('CORRECT' if correct_sp else 'WRONG')\n",
        "\n",
        "  #   total_correct_sp += correct_sp\n",
        "\n",
        "  # avg_correct_sp = total_correct_sp / num_examples\n",
        "\n",
        "  # if args.verbose:\n",
        "  #   print('validation accuracy:', avg_correct_sp*100)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "JBp0XKExQ7Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseAdam(torch.optim.Optimizer):\n",
        "  def __init__(self, params, lr=5e-5, betas=(0.9, 0.999), eps=1e-8, correct_bias=False, sparsity='row'):\n",
        "    if lr < 0.0:\n",
        "        raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
        "    if not 0.0 <= betas[0] < 1.0:\n",
        "        raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
        "    if not 0.0 <= betas[1] < 1.0:\n",
        "        raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
        "    if not 0.0 <= eps:\n",
        "        raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
        "    if sparsity != 'col' and sparsity != 'row':\n",
        "        raise ValueError(f\"Invalid sparsity value: {sparsity} - must be 'row' or 'col'\")\n",
        "\n",
        "    defaults = dict(lr=lr, betas=betas, eps=eps, correct_bias=correct_bias, sparsity=sparsity)\n",
        "    super(SparseAdam, self).__init__(params, defaults)\n",
        "\n",
        "  def step(self, indexer):\n",
        "    for group in self.param_groups:\n",
        "      for p in group[\"params\"]:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        grad = p.grad\n",
        "\n",
        "        sparsity = group[\"sparsity\"]\n",
        "        if sparsity == 'row':\n",
        "          grad[indexer] = 0 # zero out grad\n",
        "        elif sparsity == 'col':\n",
        "          grad[:, indexer] = 0\n",
        "\n",
        "        state = self.state[p]\n",
        "\n",
        "        # State initialization\n",
        "        if len(state) == 0:\n",
        "          state[\"step\"] = 0\n",
        "          # Exponential moving average of gradient values\n",
        "          state[\"m\"] = torch.zeros_like(p)\n",
        "          # Exponential moving average of squared gradient values\n",
        "          state[\"v\"] = torch.zeros_like(p)\n",
        "\n",
        "        m_tm1, v_tm1 = state[\"m\"], state[\"v\"]\n",
        "        beta1, beta2 = group[\"betas\"]\n",
        "\n",
        "        state[\"step\"] += 1\n",
        "\n",
        "        m_t = beta1 * m_tm1 + (1 - beta1) * grad\n",
        "        if sparsity == 'row':\n",
        "          m_t[indexer] = 0 # zero out m_t\n",
        "        elif sparsity == 'col':\n",
        "          m_t[:, indexer] = 0\n",
        "\n",
        "        v_t = beta2 * v_tm1 + (1 - beta2) * torch.pow(grad, 2)\n",
        "        if sparsity == 'row':\n",
        "          v_t[indexer] = 0 # zero out v_t\n",
        "        elif sparsity == 'col':\n",
        "          v_t[:, indexer] = 0\n",
        "\n",
        "        step_size = group[\"lr\"]\n",
        "        if group[\"correct_bias\"]:  # No bias correction for Bert\n",
        "          bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
        "          bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
        "          step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "        p.data = p.data - step_size * m_t / (torch.sqrt(v_t) + group[\"eps\"])\n",
        "\n",
        "        state[\"m\"] = m_t\n",
        "        state[\"v\"] = v_t"
      ],
      "metadata": {
        "id": "qGFGqfZMTwnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputDataset(Dataset):\n",
        "  def __init__(self, attn_masks, hidden_states):\n",
        "    self.attn_masks = attn_masks\n",
        "    self.hidden_states = hidden_states\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.hidden_states.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.attn_masks[idx], self.hidden_states[idx]"
      ],
      "metadata": {
        "id": "4nbbdg8iGbaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def zero_out(S, dense_layer, pruned_layer):\n",
        "  print('zeroing out')\n",
        "  heads_to_keep = S\n",
        "  heads_to_prune = set(range(layer.n_heads)).difference(heads_to_keep)\n",
        "  indexer_to_prune = get_mask_indexer(heads_to_keep, layer.n_heads, layer.attention_head_size)\n",
        "\n",
        "  for name, parameter in layer.named_parameters():\n",
        "    if 'bias' in name:\n",
        "      if 'out' in name:\n",
        "        pass\n",
        "      else:\n",
        "        parameter.data[indexer_to_prune] = 0\n",
        "    else:\n",
        "      if 'out' in name:\n",
        "        parameter.data[:, indexer_to_prune] = 0\n",
        "      else:\n",
        "        parameter.data[indexer_to_prune, :] = 0\n",
        "\n",
        "  return layer"
      ],
      "metadata": {
        "id": "2ODn5ilUs80T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transformer_arguments(\n",
        "    model,\n",
        "    input_ids = None,\n",
        "    attention_mask = None,\n",
        "    head_mask = None,\n",
        "    inputs_embeds = None,\n",
        "    output_attentions = None,\n",
        "    output_hidden_states = None,\n",
        "    return_dict = None):\n",
        "\n",
        "  output_attentions = output_attentions if output_attentions is not None else model.config.output_attentions\n",
        "  output_hidden_states = (\n",
        "      output_hidden_states if output_hidden_states is not None else model.config.output_hidden_states\n",
        "  )\n",
        "  return_dict = return_dict if return_dict is not None else model.config.use_return_dict\n",
        "\n",
        "  if input_ids is not None and inputs_embeds is not None:\n",
        "      raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "  elif input_ids is not None:\n",
        "      input_shape = input_ids.size()\n",
        "  elif inputs_embeds is not None:\n",
        "      input_shape = inputs_embeds.size()[:-1]\n",
        "  else:\n",
        "      raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "  device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "  if attention_mask is None:\n",
        "      attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
        "\n",
        "  # Prepare head mask if needed\n",
        "  head_mask = model.get_head_mask(head_mask, model.config.num_hidden_layers)\n",
        "\n",
        "  arguments = ({\n",
        "                'output_attentions': output_attentions,\n",
        "                'output_hidden_states': output_hidden_states,\n",
        "                'return_dict': return_dict,\n",
        "                'head_mask': head_mask,\n",
        "                'attn_mask': attention_mask,\n",
        "              })\n",
        "\n",
        "  return arguments"
      ],
      "metadata": {
        "id": "7Wvi88qm4MMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Up0qXPZZV4"
      },
      "outputs": [],
      "source": [
        "def get_data(d_name):\n",
        "  dataset = load_dataset(d_name)\n",
        "  train_dataset = dataset['train']\n",
        "  test_ds = dataset['validation']\n",
        "\n",
        "  split_ds = train_dataset.train_test_split(test_size=0.1)\n",
        "  train_ds = split_ds['train']\n",
        "  val_ds = split_ds['test']\n",
        "  return train_ds, val_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_mask_indexer(heads_to_keep, n_heads, head_size):\n",
        "  mask = torch.ones(n_heads, head_size)\n",
        "  for head in heads_to_keep:\n",
        "      mask[head] = 0\n",
        "  mask = mask.view(-1).contiguous().eq(1)\n",
        "  index = torch.arange(len(mask))[mask].long()\n",
        "  return index"
      ],
      "metadata": {
        "id": "uaLImWUA4tJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate heads\n",
        "def separate_heads(x, bs, n_heads, dim_per_head):\n",
        "  return x.view(bs, -1, n_heads, dim_per_head).transpose(1, 2)"
      ],
      "metadata": {
        "id": "nCdi4ji1e8zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dotdict:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            if isinstance(value, dict):\n",
        "                setattr(self, key, dotdict(value))\n",
        "            else:\n",
        "                setattr(self, key, value)"
      ],
      "metadata": {
        "id": "ztlD4tsK1-qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_objective(mat):\n",
        "    return 0.5 * torch.mean(mat**2) # using torch.sum requires small stepsize (order of 1e-5)"
      ],
      "metadata": {
        "id": "noQozIrwKQyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tuning"
      ],
      "metadata": {
        "id": "-NImY0A7mNXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_finetuning(layer, input_loader):\n",
        "  pass"
      ],
      "metadata": {
        "id": "0csXhQOHmPgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prune model"
      ],
      "metadata": {
        "id": "J4SaqDZ-t86w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNKIod7ST5Td"
      },
      "outputs": [],
      "source": [
        "def prune_distilbert(args, model=None, tokenizer=None, train_ds=None, val_ds=None, test_ds=None, device='cpu'):\n",
        "  if train_ds is None and test_ds is None and val_ds is None:\n",
        "    train_ds, val_ds, test_ds = get_data('squad')\n",
        "  if tokenizer is None:\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "  if model is None:\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "\n",
        "  questions = train_ds['question']\n",
        "  texts = train_ds['context']\n",
        "\n",
        "  inputs = tokenizer(\n",
        "        questions,\n",
        "        texts,\n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  embedder = model.distilbert.embeddings\n",
        "  transformer = model.distilbert.transformer\n",
        "\n",
        "  transformer_arguments = get_transformer_arguments(model, **inputs)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    embeddings = embedder(inputs.input_ids)\n",
        "\n",
        "  val_arguments = {}\n",
        "  if args.validate_iter or args.validate_layer:\n",
        "    assert val_ds is not None\n",
        "    start_idx = torch.randint(low=0, high=len(questions) - args.val_size, size=(1,)).item()\n",
        "\n",
        "    val_ids = val_ds['id'][start_idx : start_idx + args.val_size]\n",
        "    val_questions =  val_ds['question'][start_idx : start_idx + args.val_size]\n",
        "    val_texts =  val_ds['context'][start_idx : start_idx + args.val_size]\n",
        "    val_answers =  val_ds['answers'][start_idx : start_idx + args.val_size]\n",
        "\n",
        "    val_inputs = tokenizer(\n",
        "          val_questions,\n",
        "          val_texts,\n",
        "          max_length=512,\n",
        "          truncation=\"only_second\",\n",
        "          stride=128,\n",
        "          padding=\"max_length\",\n",
        "          return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_arguments['ids'] = val_ids\n",
        "    val_arguments['inputs'] = val_inputs\n",
        "    val_arguments['answers'] = val_answers\n",
        "    val_arguments['model'] = deepcopy(model)\n",
        "    val_arguments['metric'] = load(args.dataset)\n",
        "\n",
        "  pruned_transformer, val_mets = prune_transformer(args, transformer, embeddings, val_arguments, **transformer_arguments, device=device)\n",
        "\n",
        "  if args.save_model:\n",
        "    torch.save(pruned_transformer.state_dict(), '/content/pruned_transformer30pct.pth')\n",
        "  return val_mets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prune transformer"
      ],
      "metadata": {
        "id": "b-Lz-tIdt6VW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21TX249ITqQu"
      },
      "outputs": [],
      "source": [
        "def prune_transformer(args, transformer, hidden_states, val_arguments, validate_iter=False, attn_mask=None, output_attentions=None, head_mask=None, output_hidden_states=None, return_dict=None, device='cpu'):\n",
        "  pruned_transformer = deepcopy(transformer)\n",
        "  layers = pruned_transformer.layer\n",
        "\n",
        "  all_val_mets = {}\n",
        "  if args.validate_layer or args.validate_iter:\n",
        "    val_arguments['model'].distilbert.transformer = pruned_transformer\n",
        "    mets = validate(args, **val_arguments, device=device)\n",
        "    all_val_mets['pre-pruning'] = mets\n",
        "    print(f'Pre-pruning validation metrics = {mets}')\n",
        "\n",
        "  for i, layer in enumerate(layers[:]):\n",
        "    if i < args.min_layer or i > args.max_layer:\n",
        "      continue\n",
        "    input_ds = InputDataset(attn_mask, hidden_states)\n",
        "    input_loader = DataLoader(input_ds, num_workers=2, shuffle=True, batch_size=args.batch_size)\n",
        "\n",
        "    print(f'\\nLayer {i}')\n",
        "\n",
        "    layer.attention = prune_attn_layer(args, layer.attention, input_loader, head_mask[i], val_arguments, layer_id=i, device=device)\n",
        "\n",
        "    if args.finetune_layer:\n",
        "      if args.validate_layer:\n",
        "        val_arguments['model'].distilbert.transformer = pruned_transformer\n",
        "        mets = validate(args, **val_arguments, device=device)\n",
        "        all_val_mets[f'layer {i}'] = mets\n",
        "        print(f'Validation metrics after pruning layer {i}, before fine-tuning = {mets}')\n",
        "\n",
        "      layer = run_finetuning(layer, input_loader)\n",
        "\n",
        "    layer = layer.to(device)\n",
        "    hidden_states = hidden_states.to(device)\n",
        "    attn_mask = attn_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      hidden_states = layer(\n",
        "          x=hidden_states,\n",
        "          attn_mask=attn_mask,\n",
        "          output_attentions=output_attentions,\n",
        "          head_mask=head_mask[i]\n",
        "      )[-1]\n",
        "\n",
        "    layer = layer.cpu()\n",
        "    hidden_states = hidden_states.cpu()\n",
        "    attn_mask = attn_mask.cpu()\n",
        "\n",
        "    del input_ds\n",
        "    del input_loader\n",
        "\n",
        "    if args.validate_layer:\n",
        "      val_arguments['model'].distilbert.transformer = pruned_transformer\n",
        "      mets = validate(args, **val_arguments, device=device)\n",
        "      all_val_mets[f'layer {i}'] = mets\n",
        "      if args.finetune_layer:\n",
        "        print(f'Validation metrics after pruning layer {i}, after fine-tuning = {mets}')\n",
        "      else:\n",
        "        print(f'Validation metrics after pruning layer {i} = {mets}')\n",
        "\n",
        "    if args.validate_iter:\n",
        "      val_arguments['model'].distilbert.transformer = pruned_transformer\n",
        "\n",
        "    # print(f'after pruning : shape of W_q = {layer.attention.q_lin}, n_heads = {layer.attention.n_heads}, shape of hidden_states = {hidden_states.shape}')\n",
        "\n",
        "  if args.verbose:\n",
        "    print(pruned_transformer)\n",
        "  return pruned_transformer, all_val_mets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct-NLK-1SI8j"
      },
      "source": [
        "## prune layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijhMW7M8SObB"
      },
      "outputs": [],
      "source": [
        "def prune_attn_layer(args, layer, input_loader, head_mask, val_arguments, layer_id=None, device='cpu'):\n",
        "  n_heads_to_keep = int(layer.n_heads * args.s)\n",
        "\n",
        "  pruned_layer = deepcopy(layer)\n",
        "  Q = deepcopy(layer)\n",
        "\n",
        "  qkv_params = []\n",
        "  out_params = []\n",
        "  for name, param in Q.named_parameters():\n",
        "    if 'out' in name:\n",
        "      if 'weight' in name:\n",
        "        out_params.append(param)\n",
        "      else:\n",
        "        continue\n",
        "    else:\n",
        "      qkv_params.append(param)\n",
        "  qkv_optimizer = SparseAdam(qkv_params, sparsity='row')\n",
        "  out_optimizer = SparseAdam(out_params, sparsity='col')\n",
        "\n",
        "  S = set([])\n",
        "  for t in range(args.T):\n",
        "    if args.validate_iter and pruned_layer is not None:\n",
        "      val_arguments['model'].distilbert.transformer.layer[layer_id].attention = pruned_layer\n",
        "      acc = validate(args, **val_arguments, device=device)\n",
        "      pruned_layer = pruned_layer.to(device)\n",
        "\n",
        "    attn_mask, hidden_states = next(iter(input_loader))\n",
        "\n",
        "    attn_mask = attn_mask.to(device)\n",
        "    hidden_states = hidden_states.to(device)\n",
        "    query, key, value = hidden_states, hidden_states, hidden_states\n",
        "    layer = layer.to(device)\n",
        "\n",
        "    importance, grad_dict, loss = compute_grads(pruned_layer, layer, hidden_states, attn_mask, head_mask, S, device=device)\n",
        "\n",
        "    if args.validate_iter:\n",
        "      print(f'Iteration {t}: S = {S} | Loss = {loss} | Exact Match = {acc[\"exact_match\"]} | F1 = {acc[\"f1\"]}')\n",
        "    elif args.iter_verbose:\n",
        "      print(f'Iteration {t}: S = {S} | Loss = {loss}')\n",
        "\n",
        "    D = find_and_merge(importance, S, n_heads_to_keep, device=device)\n",
        "    Q = update_step(args, Q, layer, grad_dict, qkv_optimizer, out_optimizer, D, device=device)\n",
        "    pruned_layer, S = truncate(Q, pruned_layer, hidden_states, attn_mask, head_mask, n_heads_to_keep, device=device)\n",
        "    pruned_layer = debias(args, pruned_layer, layer, input_loader, args.debias_iters, head_mask, eta=None, device=device)\n",
        "\n",
        "    del grad_dict\n",
        "\n",
        "  del Q\n",
        "  del layer\n",
        "\n",
        "  print(f'Post-pruning: S = {S}')\n",
        "  all_heads = set(range(pruned_layer.n_heads))\n",
        "\n",
        "  if layer_id is not None:\n",
        "    print(f'Done pruning layer {layer_id}')\n",
        "\n",
        "  return pruned_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run"
      ],
      "metadata": {
        "id": "jVw1rRg3bZPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'validate_iter': False,\n",
        "    'validate_layer': True,\n",
        "    'T': 10,\n",
        "    's': 0.3,\n",
        "    'debias_iters': 5,\n",
        "    'min_layer': 0,\n",
        "    'max_layer': 10,\n",
        "    'batch_size': 16,\n",
        "    'val_size': 100,\n",
        "    'verbose': False,\n",
        "    'iter_verbose': False,\n",
        "    'dataset': 'squad',\n",
        "    'save_model': False,\n",
        "    'runs': 2,\n",
        "    'maintain_Q': False,\n",
        "    'eta': 1e-4,\n",
        "    'trunc_strategy': 'magnitude',\n",
        "    'finetune_layer': False,\n",
        "    'train_size': 200\n",
        "}\n",
        "args = dotdict(args)"
      ],
      "metadata": {
        "id": "fo338LR71j1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "# train_ds, val_ds, test_ds = get_data(args.dataset)\n",
        "# train_ds = train_ds[:args.train_size]"
      ],
      "metadata": {
        "id": "zbN7CEvhitEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(device)\n",
        "start_time = time.time()\n",
        "\n",
        "all_run_mets = []\n",
        "for i in range(args.runs):\n",
        "  print(f'\\nRun {i}')\n",
        "  train_ds, val_ds, test_ds = get_data(args.dataset)\n",
        "  train_ds = train_ds[:args.train_size]\n",
        "  run_mets = prune_distilbert(args, model, tokenizer, train_ds, test_ds, device=device)\n",
        "  all_run_mets.append(run_mets)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f'average run time = {(end_time - start_time) / args.runs}s')\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "k08tKxJFp1dA",
        "outputId": "11250e23-63a9-46df-ea89-fd70da5bd250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "\n",
            "Run 0\n",
            "Pre-pruning validation metrics = {'exact_match': 72.0, 'f1': 84.0200495696003}\n",
            "\n",
            "Layer 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-0df3887fc0a8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mrun_mets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_distilbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mall_run_mets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_mets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-6295d1ab3e83>\u001b[0m in \u001b[0;36mprune_distilbert\u001b[0;34m(args, model, tokenizer, train_ds, val_ds, test_ds, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mval_arguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpruned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtransformer_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a742afa057ff>\u001b[0m in \u001b[0;36mprune_transformer\u001b[0;34m(args, transformer, hidden_states, val_arguments, validate_iter, attn_mask, output_attentions, head_mask, output_hidden_states, return_dict, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nLayer {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_attn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-1665d01da513>\u001b[0m in \u001b[0;36mprune_attn_layer\u001b[0;34m(args, layer, input_loader, head_mask, val_arguments, layer_id, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpruned_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruned_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mpruned_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruned_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebias_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a3145933cbbb>\u001b[0m in \u001b[0;36mdebias\u001b[0;34m(args, pruned_layer, dense_layer, input_loader, iters, head_mask, eta, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdense_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mU_pruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpruned_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mU_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_reshp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         scores = scores.masked_fill(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         )  # (bs, n_heads, q_length, k_length)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected self and mask to be on the same device, but got mask on cpu and self on cuda:0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_run_mets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsRNGNt8kvdo",
        "outputId": "40d77c61-b68e-4b57-b58a-769dc5ace7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'pre-pruning': {'exact_match': 72.0, 'f1': 83.27004956960029},\n",
              "  'layer 0': {'exact_match': 66.0, 'f1': 79.0994645705172},\n",
              "  'layer 1': {'exact_match': 33.0, 'f1': 43.54160187197602},\n",
              "  'layer 2': {'exact_match': 25.0, 'f1': 34.97856412921605},\n",
              "  'layer 3': {'exact_match': 10.0, 'f1': 20.77237946338364},\n",
              "  'layer 4': {'exact_match': 7.0, 'f1': 11.01484083604468},\n",
              "  'layer 5': {'exact_match': 8.0, 'f1': 13.723320954571664}},\n",
              " {'pre-pruning': {'exact_match': 75.0, 'f1': 83.921978021978},\n",
              "  'layer 0': {'exact_match': 67.0, 'f1': 77.29047619047618},\n",
              "  'layer 1': {'exact_match': 46.0, 'f1': 58.2446301807476},\n",
              "  'layer 2': {'exact_match': 39.0, 'f1': 46.2746657136901},\n",
              "  'layer 3': {'exact_match': 11.0, 'f1': 16.147464060418937},\n",
              "  'layer 4': {'exact_match': 8.0, 'f1': 9.77171589870085},\n",
              "  'layer 5': {'exact_match': 10.0, 'f1': 15.003244172268563}},\n",
              " {'pre-pruning': {'exact_match': 72.0, 'f1': 83.27004956960029},\n",
              "  'layer 0': {'exact_match': 66.0, 'f1': 79.0994645705172},\n",
              "  'layer 1': {'exact_match': 33.0, 'f1': 43.54160187197602},\n",
              "  'layer 2': {'exact_match': 24.0, 'f1': 33.97856412921605},\n",
              "  'layer 3': {'exact_match': 10.0, 'f1': 20.840561281565456},\n",
              "  'layer 4': {'exact_match': 7.0, 'f1': 11.26484083604468},\n",
              "  'layer 5': {'exact_match': 8.0, 'f1': 13.89924688049759}},\n",
              " {'pre-pruning': {'exact_match': 73.0, 'f1': 84.27004956960029},\n",
              "  'layer 0': {'exact_match': 66.0, 'f1': 79.0994645705172},\n",
              "  'layer 1': {'exact_match': 33.0, 'f1': 43.5903823597809},\n",
              "  'layer 2': {'exact_match': 25.0, 'f1': 34.97856412921605},\n",
              "  'layer 3': {'exact_match': 10.0, 'f1': 20.75217744318162},\n",
              "  'layer 4': {'exact_match': 7.0, 'f1': 10.99463881584266},\n",
              "  'layer 5': {'exact_match': 8.0, 'f1': 13.723320954571664}},\n",
              " {'pre-pruning': {'exact_match': 72.0, 'f1': 82.92658043710674},\n",
              "  'layer 0': {'exact_match': 66.0, 'f1': 78.65898838004101},\n",
              "  'layer 1': {'exact_match': 40.0, 'f1': 54.49055231771833},\n",
              "  'layer 2': {'exact_match': 36.0, 'f1': 46.05049298239663},\n",
              "  'layer 3': {'exact_match': 11.0, 'f1': 19.219924932879806},\n",
              "  'layer 4': {'exact_match': 8.0, 'f1': 9.77171589870085},\n",
              "  'layer 5': {'exact_match': 10.0, 'f1': 15.003244172268563}}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plots"
      ],
      "metadata": {
        "id": "WIiuJ0yvJJq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mets_by_score = []\n",
        "for run in all_run_mets: # all_run_mets is an array of dicts, each run should be a dict\n",
        "  metrics = run['pre-pruning'].keys()\n",
        "  run_metrics = {met: [0 for _ in run.keys()] for met in metrics}\n",
        "  for i, layer in enumerate(run.keys()):\n",
        "    for met in metrics:\n",
        "      run_metrics[met][i] = run[layer][met]\n",
        "  mets_by_score.append(run_metrics)\n",
        "mets_by_score"
      ],
      "metadata": {
        "id": "BPnyGllnlcdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = list(mets_by_score[0].keys())\n",
        "for met in metrics:\n",
        "  for i, exp in enumerate(mets_by_score):\n",
        "    data = exp[met]\n",
        "    plt.plot(data, label=f\"Run {i}\")\n",
        "  plt.legend()\n",
        "  plt.ylabel('score')\n",
        "  plt.xlabel('before pruning layer')\n",
        "  plt.title(f'implementation (c), {met} by run')\n",
        "  plt.show()\n",
        "# for exp in avgs_by_exp:"
      ],
      "metadata": {
        "id": "tZgFs85le-Tl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O3qZ8rlt8Qpz",
        "zDSlbyIb8WAA",
        "skR8mj0c8c7i",
        "9guWG7Dd27KE",
        "VdryzDILQYua",
        "8EN5SnK6QiTb",
        "K0mGPnklQ6RB",
        "Mzzs5h5eRbBZ",
        "1il8obD1Ru_C",
        "IHk6P4I8Gxdw",
        "Ct-NLK-1SI8j",
        "ESK7q33TVq93"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
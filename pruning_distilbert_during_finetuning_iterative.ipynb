{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RNOxqhLOcD9a",
        "tNXnlzDYycri",
        "U-l0PRfFyenw",
        "bkTKpTdEQPaF",
        "VdryzDILQYua",
        "K0mGPnklQ6RB",
        "Mzzs5h5eRbBZ",
        "1il8obD1Ru_C",
        "iIF7YP0Hyni3",
        "0YrHHG_XAXOB",
        "5uzqg4OgAbEY",
        "TegeTJT-AYkN",
        "Bj7kwd_xylNW",
        "qI7fIncYepgW",
        "OStkavXOypi-",
        "XbdADMgGytn6",
        "2WKY_r5_yic0"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# mount drive"
      ],
      "metadata": {
        "id": "RNOxqhLOcD9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "1lvOUfN5cFE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "tNXnlzDYycri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate"
      ],
      "metadata": {
        "id": "Y-oHq9Kby0PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "LJh-pxJVy1l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "import math\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from copy import deepcopy\n",
        "import time\n",
        "import json\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    DistilBertForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    DistilBertTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions\n",
        ")\n",
        "from transformers.modeling_utils import apply_chunking_to_forward\n",
        "from transformers.trainer_utils import SchedulerType\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version\n",
        "from transformers.utils.versions import require_version\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "EXO_WWgIxjcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_min_version(\"4.16.0.dev0\")\n",
        "\n",
        "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "puCM_-l8xmM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training argument classes"
      ],
      "metadata": {
        "id": "U-l0PRfFyenw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    Using `HfArgumentParser` we can turn this class\n",
        "    into argparse arguments to be able to specify them on\n",
        "    the command line.\n",
        "    \"\"\"\n",
        "\n",
        "    max_seq_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "            \"than this will be truncated, sequences shorter will be padded.\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
        "    )\n",
        "    pad_to_max_length: bool = field(\n",
        "        default=True,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
        "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n",
        "    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})"
      ],
      "metadata": {
        "id": "QE-6KkbLxpc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    language: str = field(\n",
        "        default=None, metadata={\"help\": \"Evaluation language. Also train language if `train_language` is set to None.\"}\n",
        "    )\n",
        "    train_language: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Train language if it is different from the evaluation language.\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    do_lower_case: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
        "            \"with private models).\"\n",
        "        },\n",
        "    )"
      ],
      "metadata": {
        "id": "hyvzGLGXxtQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkTKpTdEQPaF"
      },
      "source": [
        "# pruning algorithm steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdryzDILQYua"
      },
      "source": [
        "### step 1: compute gradient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_grads(args, dense_layer, pruned_layer, H, weight_indexer, attn_mask, head_mask=None, device='cpu'):\n",
        "  dense_layer.zero_grad()\n",
        "  pruned_layer.zero_grad()\n",
        "\n",
        "  if weight_indexer is None:\n",
        "    U_dense = dense_layer(H, H, H, attn_mask, head_mask=head_mask, output_attentions=False)[0]\n",
        "    loss = residual_objective(U_dense)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      dense_grad_dict = {}\n",
        "      for name, parameter in dense_layer.named_parameters():\n",
        "        dense_grad_dict[name] = parameter.grad\n",
        "\n",
        "    return dense_grad_dict, loss.item()\n",
        "\n",
        "  else:\n",
        "    U_dense = dense_layer(H, H, H, attn_mask, head_mask=head_mask, output_attentions=False)[0]\n",
        "    U_pruned = pruned_layer(H, H, H, attn_mask, head_mask=head_mask, output_attentions=False)[0]\n",
        "    loss = residual_objective(U_dense - U_pruned)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pruned_grad_dict = {}\n",
        "      for name, parameter in pruned_layer.named_parameters():\n",
        "        pruned_grad_dict[name] = parameter.grad\n",
        "\n",
        "      dense_grad_dict = {}\n",
        "      for name, parameter in dense_layer.named_parameters():\n",
        "        dense_grad_dict[name] = parameter.grad\n",
        "\n",
        "  return pruned_grad_dict, dense_grad_dict, loss.item()"
      ],
      "metadata": {
        "id": "kF1oycHfpLF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0mGPnklQ6RB"
      },
      "source": [
        "### step 2: update Q by gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWNeUbvIQ5zD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def update_step(args, Q, grad_dict, S_indexer, device='cpu'):\n",
        "  if S_indexer is None:\n",
        "    for name, Q_param in Q.named_parameters():\n",
        "      assert name in grad_dict, f\"parameter {name} does not have grad\"\n",
        "      grad = grad_dict[name]\n",
        "      Q_param.data = Q_param.data - args.eta * grad\n",
        "\n",
        "  else:\n",
        "    for name, Q_param in Q.named_parameters():\n",
        "      assert name in grad_dict, f\"parameter {name} does not have grad\"\n",
        "      grad = grad_dict[name]\n",
        "      if 'out' in name:\n",
        "        if 'weight' in name:\n",
        "          Q_param.data[:, S_indexer] = Q_param.data[:, S_indexer] - args.eta * grad\n",
        "        else:\n",
        "          Q_param.data = Q_param.data - args.eta * grad\n",
        "      else:\n",
        "        Q_param.data[S_indexer] = Q_param.data[S_indexer] - args.eta * grad\n",
        "\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzzs5h5eRbBZ"
      },
      "source": [
        "### step 3: calculate importance scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWLNhMImRgwn"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def calc_importance_by_magnitude(args, Q, device='cpu'): # truncate based on weights? or combine with WANDA? or truncate based on output? create args.trunc_strategy and try all of these.\n",
        "  importance = None\n",
        "  for name, param in Q.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      if 'out' in name:\n",
        "        param_reshaped = param.view(Q.dim, Q.n_heads, Q.attention_head_size)\n",
        "        imp_by_head = torch.norm(param_reshaped, p=2, dim=(0,2))\n",
        "      else:\n",
        "        param_reshaped = param.view(Q.n_heads, Q.attention_head_size, Q.dim)\n",
        "        imp_by_head = torch.norm(param_reshaped, p=2, dim=(1,2))\n",
        "      if importance is None:\n",
        "        importance = torch.zeros_like(imp_by_head)\n",
        "      importance += imp_by_head\n",
        "\n",
        "  return importance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def calc_importance_by_attn_scores(args, layer, hidden_states, attn_mask, head_mask=None, device='cpu'): # truncate based on weights? or combine with WANDA? or truncate based on output? create args.trunc_strategy and try all of these.\n",
        "  def shape(x):\n",
        "    \"\"\"separate heads\"\"\"\n",
        "    return x.view(x.shape[0], -1, layer.n_heads, layer.attention_head_size).transpose(1, 2)\n",
        "\n",
        "  def unshape(x):\n",
        "    \"\"\"group heads\"\"\"\n",
        "    return x.transpose(1, 2).contiguous().view(x.shape[0], -1, layer.n_heads * layer.attention_head_size)\n",
        "\n",
        "  attn_mask = attn_mask.to(device)\n",
        "  hidden_states = hidden_states.to(device)\n",
        "  query, key, value = hidden_states, hidden_states, hidden_states\n",
        "\n",
        "  _, weights = layer(query, key, value, attn_mask, head_mask=head_mask, output_attentions=True)\n",
        "\n",
        "  v = shape(layer.v_lin(value))\n",
        "  context = torch.matmul(weights, v)\n",
        "\n",
        "  H = unshape(context)\n",
        "  importance = torch.norm(context, p=2, dim=(0, 2, 3))\n",
        "\n",
        "  return importance"
      ],
      "metadata": {
        "id": "yvch8bue6JHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step 3.5: find best s columns outside of S and merge with S"
      ],
      "metadata": {
        "id": "bvfSD4AIJUgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_and_merge(dense_layer, grad_dict, S, n_heads_to_keep):\n",
        "  importance = None\n",
        "  with torch.no_grad():\n",
        "    for name in grad_dict:\n",
        "      grad = grad_dict[name]\n",
        "      if 'weight' in name:\n",
        "        if 'out' in name:\n",
        "          grad_reshaped = grad.view(-1, dense_layer.n_heads, dense_layer.attention_head_size)\n",
        "          imp_by_head = torch.norm(grad_reshaped, p=2, dim=(0,2))\n",
        "        else:\n",
        "          grad_reshaped = grad.view(dense_layer.n_heads, dense_layer.attention_head_size, -1)\n",
        "          imp_by_head = torch.norm(grad_reshaped, p=2, dim=(1,2))\n",
        "        if importance is None:\n",
        "          importance = torch.zeros_like(imp_by_head)\n",
        "        importance += imp_by_head\n",
        "\n",
        "  for index in range(len(importance)):\n",
        "    if index in S:\n",
        "      importance[index] = 0\n",
        "\n",
        "  imp_top_idxs = torch.argsort(importance, descending=True)[:n_heads_to_keep]\n",
        "  imp_top_idxs = set(imp_top_idxs.cpu().tolist())\n",
        "  D = S.union(imp_top_idxs)\n",
        "\n",
        "  return D"
      ],
      "metadata": {
        "id": "2RaJMsiyJqYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step 4: truncate"
      ],
      "metadata": {
        "id": "JTuYOAZO6Ec4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def truncate(args, pruned_layer, Q, importance, D, n_heads_to_keep):\n",
        "  # print(f'importance for truncation: {sorted([float(imp) for imp in importance])}')\n",
        "  # print(f'importance for truncation with D: {([float(imp) for imp in importance])}')\n",
        "\n",
        "  # for index in range(len(importance1)):\n",
        "  #   if index in S:\n",
        "  #     importance1[index] = 0\n",
        "  # imp1_top_idxs = torch.argsort(importance1, descending=True)[:n_heads_to_keep]\n",
        "  # D = S.union(set(imp1_top_idxs.cpu().tolist()))\n",
        "\n",
        "  if args.choose_from_D:\n",
        "    for index in range(len(importance)):\n",
        "      if index not in D:\n",
        "        importance[index] = 0\n",
        "\n",
        "  # print(f'D: {D}, importance for truncation without D: {([float(imp) for imp in importance])}')\n",
        "\n",
        "  imp2_top_idxs = torch.argsort(importance, descending=True)[:n_heads_to_keep]\n",
        "  S = set(imp2_top_idxs.cpu().tolist())\n",
        "\n",
        "  # imp_top_idxs = torch.argsort(importance, descending=True)[:n_heads_to_keep]\n",
        "  # S = set(imp_top_idxs.cpu().tolist())\n",
        "\n",
        "  pruned_layer = deepcopy(Q)\n",
        "  not_S = set(range(Q.n_heads)).difference(S)\n",
        "  pruned_layer.prune_heads(not_S)\n",
        "  print(not_S)\n",
        "\n",
        "  S_indexer = get_weight_indexer(S, Q.attention_head_size)\n",
        "\n",
        "  return pruned_layer, S, S_indexer"
      ],
      "metadata": {
        "id": "5NUe9ty66GYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1il8obD1Ru_C"
      },
      "source": [
        "### step 5: debias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvcRPgoWRxNg"
      },
      "outputs": [],
      "source": [
        "def debias(args, pruned_layer, dense_layer, Q, iters, H, attn_mask, weight_indexer, head_mask=None, device='cpu'):\n",
        "  # print(f'Q: {Q}')\n",
        "  # print(f'pruned layer: {pruned_layer}')\n",
        "  # print(f'weight indexer: {weight_indexer.shape}')\n",
        "  with torch.no_grad():\n",
        "    U_dense = dense_layer(H, H, H, attn_mask, head_mask=head_mask, output_attentions=False)[0]\n",
        "\n",
        "  optimizer = optim.SGD(pruned_layer.parameters(), lr=args.debias_eta)\n",
        "  for t in range(iters):\n",
        "    U_pruned = pruned_layer(H, H, H, attn_mask, head_mask=head_mask, output_attentions=False)[0]\n",
        "\n",
        "    loss = residual_objective(U_dense.detach() - U_pruned)\n",
        "    loss.backward()\n",
        "\n",
        "    # print(f'loss: {loss.item()}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      optimizer.step()\n",
        "\n",
        "    pruned_layer.zero_grad()\n",
        "    dense_layer.zero_grad()\n",
        "\n",
        "  if args.maintain_Q:\n",
        "    for (Q_name, Q_param), (W_name, W_param) in zip(Q.named_parameters(), pruned_layer.named_parameters()):\n",
        "      assert Q_name == W_name\n",
        "      if 'out' in Q_name:\n",
        "        if 'weight' in Q_name:\n",
        "          Q_param.data[:, weight_indexer] = W_param.data\n",
        "      else:\n",
        "          Q_param.data[weight_indexer] = W_param.data\n",
        "\n",
        "  return pruned_layer, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "iIF7YP0Hyni3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ispasp"
      ],
      "metadata": {
        "id": "XuDBd9WmvF92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heads_to_indexer(head_list, head_dim):\n",
        "    indices = []\n",
        "    for i in head_list:\n",
        "        indices.extend(list(range(head_dim*i, head_dim*(i + 1))))\n",
        "    indices = sorted(indices)\n",
        "    return torch.LongTensor(indices)"
      ],
      "metadata": {
        "id": "X1r8x3iIvE1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(model, data):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    iids = data['input_ids'].to(device)\n",
        "    ttids = data['token_type_ids'].to(device)\n",
        "    att_mask = data['attention_mask'].to(device)\n",
        "    ext_att_mask = model.bert.get_extended_attention_mask(att_mask, iids.size(), device)\n",
        "    #head_mask = model.bert.get_head_mask(None, model.bert.config.num_hidden_layers)\n",
        "    embedding_output = model.bert.embeddings(input_ids=iids, token_type_ids=ttids)\n",
        "    return embedding_output, ext_att_mask"
      ],
      "metadata": {
        "id": "3pbnXBl4vSZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_objective_ispasp(mat):\n",
        "    return 0.5 * torch.sum(mat**2)"
      ],
      "metadata": {
        "id": "5p5Xkd8-0ARj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## general"
      ],
      "metadata": {
        "id": "0YrHHG_XAXOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dotdict:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            if isinstance(value, dict):\n",
        "                setattr(self, key, dotdict(value))\n",
        "            else:\n",
        "                setattr(self, key, value)"
      ],
      "metadata": {
        "id": "z45lWKWc5bIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrn(ckpt_path, model):\n",
        "    print(f'\\n\\nLoading model from: {ckpt_path}\\n\\n')\n",
        "    #CONFIG_NAME = 'config.json'\n",
        "    WEIGHTS_NAME = 'pytorch_model.bin'\n",
        "    #cfg_path = os.path.join(ckpt_path, CONFIG_NAME)\n",
        "    weight_path = os.path.join(ckpt_path, WEIGHTS_NAME)\n",
        "    assert os.path.exists(weight_path)\n",
        "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state_dict)\n",
        "    del state_dict\n",
        "    return model"
      ],
      "metadata": {
        "id": "gC7CduCAyFDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transformer_arguments(model, data, device='cpu'):\n",
        "  model = model.to(device)\n",
        "  input_ids = data['input_ids'].to(device)\n",
        "  with torch.no_grad():\n",
        "    embeddings = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "  input_shape = input_ids.size()\n",
        "\n",
        "  attention_mask = data['attention_mask']\n",
        "  if attention_mask is None:\n",
        "      attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
        "\n",
        "  return embeddings, attention_mask"
      ],
      "metadata": {
        "id": "uNxYtU8b1OvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pruning"
      ],
      "metadata": {
        "id": "5uzqg4OgAbEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate heads\n",
        "def separate_heads(x, bs, n_heads, dim_per_head):\n",
        "  return x.view(bs, -1, n_heads, dim_per_head).transpose(1, 2)"
      ],
      "metadata": {
        "id": "7Uwxumfi2mdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight_indexer(head_list, head_dim):\n",
        "  indices = []\n",
        "  for i in head_list:\n",
        "      indices.extend(list(range(head_dim*i, head_dim*(i + 1))))\n",
        "  indices = sorted(indices)\n",
        "  return torch.LongTensor(indices)"
      ],
      "metadata": {
        "id": "Gq4wCvvn2lFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_objective(mat):\n",
        "    return 0.5 * torch.norm(mat) # using torch.sum requires small stepsize (order of 1e-5)"
      ],
      "metadata": {
        "id": "sAU4XOtW2kNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(model, data):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    iids = data['input_ids'].to(device)\n",
        "    ttids = data['token_type_ids'].to(device)\n",
        "    att_mask = data['attention_mask'].to(device)\n",
        "    ext_att_mask = model.bert.get_extended_attention_mask(att_mask, iids.size(), device)\n",
        "    #head_mask = model.bert.get_head_mask(None, model.bert.config.num_hidden_layers)\n",
        "    embedding_output = model.bert.embeddings(input_ids=iids, token_type_ids=ttids)\n",
        "    return embedding_output, ext_att_mask"
      ],
      "metadata": {
        "id": "RbkusaY4zEB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "TegeTJT-AYkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples, tokenizer):\n",
        "    # Tokenize the texts\n",
        "    inputs = tokenizer(\n",
        "      examples['question'],\n",
        "      examples[\"context\"],\n",
        "      max_length=512,\n",
        "      truncation=\"only_second\",\n",
        "      stride=128,\n",
        "      padding=\"max_length\",\n",
        "      return_tensors='pt',\n",
        "      return_offsets_mapping=True,\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "wwfZagaK43tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# finetune"
      ],
      "metadata": {
        "id": "Bj7kwd_xylNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval(args, model, train_dict, val_arguments, epochs, save_path=None, save_model=False, lr=5e-5, lr_decay=True, device='cpu'):\n",
        "    model.eval()\n",
        "    val_arguments['model'] = model\n",
        "    metrics_before_ft = validate(args, **val_arguments, device=device)\n",
        "    print(f'before finetuning: {metrics_before_ft}')\n",
        "\n",
        "    if epochs > 0:\n",
        "      train_dict['args'].learning_rate = lr\n",
        "      if lr_decay:\n",
        "          train_dict['args'].lr_scheduler_type = SchedulerType.LINEAR\n",
        "      else:\n",
        "          train_dict['args'].lr_scheduler_type = SchedulerType.CONSTANT\n",
        "      train_dict['args'].num_train_epochs = float(epochs)\n",
        "      trainer = Trainer(model=model, **train_dict)\n",
        "      model.train()\n",
        "\n",
        "      if args.freeze_encoder:\n",
        "        for param in model.distilbert.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      train_result = trainer.train(resume_from_checkpoint=None)\n",
        "      metrics = train_result.metrics\n",
        "      max_train_samples = len(train_dict['train_dataset'])\n",
        "      metrics[\"train_samples\"] = max_train_samples\n",
        "      if save_path is not None:\n",
        "          trainer.output_dir = save_path\n",
        "          trainer.run_name = save_path\n",
        "          if save_model:\n",
        "              trainer.save_model()\n",
        "          trainer.log_metrics(\"train\", metrics)\n",
        "          trainer.save_metrics(\"train\", metrics)\n",
        "          trainer.save_state()\n",
        "\n",
        "    model.eval()\n",
        "    val_arguments['model'] = model\n",
        "    metrics_after_ft = validate(args, **val_arguments, device=device)\n",
        "    print(f'after finetuning: {metrics_after_ft}')\n",
        "    return model, (metrics_before_ft, metrics_after_ft)\n"
      ],
      "metadata": {
        "id": "WJ2164FJyGnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validate"
      ],
      "metadata": {
        "id": "qI7fIncYepgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(args, model, ids, inputs, answers, metric, tokenizer, device='cpu'):\n",
        "  model = model.to(device)\n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad():\n",
        "    start_sp = time.time()\n",
        "    outputs_sp = model(**inputs)\n",
        "    end_sp = time.time()\n",
        "    if args.verbose:\n",
        "      print(f'done predicting using sparse model. time elapsed = {end_sp - start_sp}s')\n",
        "\n",
        "  num_examples = len(ids)\n",
        "  preds = []\n",
        "  refs = []\n",
        "  for i in range(num_examples):\n",
        "    answer_start_index_sp = torch.argmax(outputs_sp.start_logits[i])\n",
        "    answer_end_index_sp = torch.argmax(outputs_sp.end_logits[i])\n",
        "    predict_answer_tokens_sp = inputs.input_ids[i, answer_start_index_sp : answer_end_index_sp + 1]\n",
        "    pred_sp = tokenizer.decode(predict_answer_tokens_sp)\n",
        "\n",
        "    pred = {'id': ids[i], 'prediction_text': pred_sp}\n",
        "    preds.append(pred)\n",
        "    ref = {'answers': answers[i], 'id': ids[i]}\n",
        "    refs.append(ref)\n",
        "\n",
        "  results = metric.compute(predictions=preds, references=refs)\n",
        "  return results"
      ],
      "metadata": {
        "id": "JBp0XKExQ7Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_val_arguments(args, val_ds, model, tokenizer):\n",
        "      val_arguments = {}\n",
        "      start_idx = torch.randint(low=0, high=len(val_ds['id']) - args.val_size, size=(1,)).item()\n",
        "\n",
        "      val_ids = val_ds['id'][start_idx : start_idx + args.val_size]\n",
        "      val_questions =  val_ds['question'][start_idx : start_idx + args.val_size]\n",
        "      val_texts =  val_ds['context'][start_idx : start_idx + args.val_size]\n",
        "      val_answers =  val_ds['answers'][start_idx : start_idx + args.val_size]\n",
        "\n",
        "      val_inputs = tokenizer(\n",
        "            val_questions,\n",
        "            val_texts,\n",
        "            max_length=512,\n",
        "            truncation=\"only_second\",\n",
        "            stride=128,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors='pt'\n",
        "      )\n",
        "\n",
        "      val_arguments['ids'] = val_ids\n",
        "      val_arguments['inputs'] = val_inputs\n",
        "      val_arguments['answers'] = val_answers\n",
        "      val_arguments['model'] = deepcopy(model)\n",
        "      val_arguments['metric'] = evaluate.load(args.dataset_name)\n",
        "      val_arguments['tokenizer'] = tokenizer\n",
        "\n",
        "      return val_arguments"
      ],
      "metadata": {
        "id": "A-77DUb7_McO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# comparisons"
      ],
      "metadata": {
        "id": "OStkavXOypi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ispasp"
      ],
      "metadata": {
        "id": "pO7Nw59Avf0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_layer_ispasp(blay, hidden_states_list, mask_list, num_iter=10, prune_ratio=0.5, layer_id=None):\n",
        "    if layer_id is not None:\n",
        "      print(f'\\nPruning layer {layer_id}')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def shape(x):\n",
        "      \"\"\"separate heads\"\"\"\n",
        "      return x.view(x.shape[0], -1, blay.attention.n_heads, blay.attention.attention_head_size).transpose(1, 2)\n",
        "\n",
        "    def unshape(x):\n",
        "      \"\"\"group heads\"\"\"\n",
        "      return x.transpose(1, 2).contiguous().view(x.shape[0], -1, blay.attention.n_heads * blay.attention.attention_head_size)\n",
        "\n",
        "    # pruning will occur a the level of the self attention module\n",
        "    sout = blay.attention.out_lin\n",
        "\n",
        "    # generate the linear layer used to do pruning stuff\n",
        "    num_prune_heads = int(blay.attention.n_heads*prune_ratio)\n",
        "    # print(f'Num prune heads: {num_prune_heads}')\n",
        "    pdense = torch.nn.Linear(\n",
        "            in_features=int(blay.attention.attention_head_size*num_prune_heads),\n",
        "            out_features=blay.attention.dim, bias=True)\n",
        "\n",
        "    # generate self attention output for each head\n",
        "    # this acts as the hidden representation used as reference for pruning\n",
        "    og_size = None\n",
        "    satt_out = []\n",
        "    hidden_states = torch.cat(hidden_states_list, dim=0)\n",
        "    attn_mask = torch.cat(mask_list, dim=0)\n",
        "    with torch.no_grad():\n",
        "      attn_mask = attn_mask.to(device)\n",
        "      hidden_states = hidden_states.to(device)\n",
        "      query, key, value = hidden_states, hidden_states, hidden_states\n",
        "\n",
        "      _, weights = blay.attention(query, key, value, attn_mask, head_mask=None, output_attentions=True)\n",
        "\n",
        "      v = shape(blay.attention.v_lin(value))\n",
        "      context = torch.matmul(weights, v)\n",
        "\n",
        "      context = context.permute(0, 2, 1, 3).contiguous()\n",
        "      # satt_out = unshape(context)\n",
        "\n",
        "      if og_size is None:\n",
        "          og_size = context.size() # store to resize importances later\n",
        "      new_shape = context.size()[:-2] + (blay.attention.dim,)\n",
        "      context = context.view(*new_shape) # [32, 128, 768], THIS IS THE OUTPUT OF SELF ATTN\n",
        "      satt_out.append(context.cpu())\n",
        "\n",
        "    batch_size = og_size[0]\n",
        "    with torch.no_grad():\n",
        "        satt_out = torch.cat(satt_out, dim=0) # [N, 128, 768]\n",
        "        aggh_shape = (satt_out.size()[0],) + og_size[1:]\n",
        "        agg_hidden = satt_out.view(*aggh_shape) # [N, 128, 12, 64]\n",
        "        agg_hidden = torch.sum(agg_hidden, dim=(0, 1, 3))\n",
        "\n",
        "    # main pruning loop\n",
        "    pruned_indices = set([])\n",
        "    weight_indexer = None\n",
        "    for t in range(num_iter):\n",
        "        importance = None\n",
        "        for i in range(0, satt_out.shape[0], batch_size):\n",
        "            # track gradient on input to the MLP\n",
        "            so = satt_out[i: i + batch_size, :]\n",
        "            so = so.to(device)\n",
        "            so.requires_grad = True\n",
        "\n",
        "            # compute dense output while tracking gradient\n",
        "            dense_out = sout(so)\n",
        "            #out = sout.LayerNorm(out + data), we don't use residual/layernorm here\n",
        "\n",
        "            # compute the pruning residual\n",
        "            if len(pruned_indices) > 0:\n",
        "                with torch.no_grad():\n",
        "                    pruned_out = pdense(so[:, :, weight_indexer])\n",
        "                residual = residual_objective(dense_out - pruned_out)\n",
        "                residual.backward()\n",
        "            else:\n",
        "                residual = residual_objective(dense_out)\n",
        "                residual.backward()\n",
        "\n",
        "            # compute importance using gradient on the input\n",
        "            tmp_imp = so.grad.detach().cpu().view(*og_size)\n",
        "            with torch.no_grad():\n",
        "                tmp_imp = torch.sum(tmp_imp, dim=(0, 1, 3))\n",
        "                if importance is None:\n",
        "                    importance = tmp_imp\n",
        "                else:\n",
        "                    importance += tmp_imp\n",
        "\n",
        "            so.grad = None\n",
        "            sout.zero_grad()\n",
        "            pdense.zero_grad()\n",
        "\n",
        "        # find most important attention heads, merge with previous active set, then threshold\n",
        "        with torch.no_grad():\n",
        "            imp_idxs = torch.argsort(importance, descending=True)[:2*num_prune_heads]\n",
        "            tmp_imp_heads = set(imp_idxs.cpu().tolist())\n",
        "            bigger_set = tmp_imp_heads.union(pruned_indices)\n",
        "            indexer = torch.LongTensor(sorted(list(bigger_set)))\n",
        "            hidden_sizes = agg_hidden[indexer]\n",
        "            # hidden_sizes = agg_hidden\n",
        "            new_pruned_indices = torch.argsort(hidden_sizes, descending=True)[:num_prune_heads]\n",
        "            new_pruned_indices = set(indexer[new_pruned_indices].cpu().tolist())\n",
        "            # new_pruned_indices = set(new_pruned_indices.cpu().tolist())\n",
        "            pruned_indices = new_pruned_indices\n",
        "\n",
        "            # copy weights into the new model\n",
        "            weight_indexer = heads_to_indexer(pruned_indices, blay.attention.attention_head_size)\n",
        "            pdense.weight.data = sout.weight.data[:, weight_indexer]\n",
        "            pdense.bias.data = sout.bias.data\n",
        "\n",
        "    print(f'Post-pruning: S = {pruned_indices}')\n",
        "\n",
        "    heads_to_prune = [x for x in range(blay.attention.n_heads) if not x in pruned_indices]\n",
        "    blay.attention.prune_heads(heads_to_prune)\n",
        "\n",
        "    if layer_id is not None:\n",
        "      print(f'Done pruning layer {layer_id}')\n",
        "    return blay, pruned_indices"
      ],
      "metadata": {
        "id": "psxM0AZ-vhFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### random"
      ],
      "metadata": {
        "id": "Tha1zOEsvloD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_layer_random_prune(blay, prune_ratio=0.5):\n",
        "    satt = blay.attention\n",
        "    num_heads_to_prune = int((1 - prune_ratio)*satt.n_heads)\n",
        "    heads_to_prune = list(range(satt.n_heads))\n",
        "    random.shuffle(heads_to_prune)\n",
        "    heads_to_prune = heads_to_prune[:num_heads_to_prune]\n",
        "    blay.attention.prune_heads(heads_to_prune)\n",
        "    return blay"
      ],
      "metadata": {
        "id": "WwEvzk73yUHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### global masking"
      ],
      "metadata": {
        "id": "VktFtd9dvnYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_global_masking(model, dl, num_batches=10, ratio=0.5):\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    num_layers = len(model.bert.encoder.layer)\n",
        "    num_heads = model.bert.encoder.layer[0].attention.self.num_attention_heads\n",
        "    final_num_heads = int(ratio * num_layers * num_heads)\n",
        "    mask_tensor = torch.ones(int(num_heads*num_layers))\n",
        "    mask_tensor = mask_tensor.to(device)\n",
        "    mask_tensor.requires_grad = True\n",
        "    for b in range(num_batches):\n",
        "        data_in = next(iter(dl))\n",
        "        labels = data_in['labels']\n",
        "        labels = labels.to(device)\n",
        "        hidden_rep, ext_att_mask = get_bert_embeddings(model, data_in)\n",
        "\n",
        "        # pass data thru each layer of the model\n",
        "        for i in range(num_layers):\n",
        "            blay = model.bert.encoder.layer[i]\n",
        "            satt = blay.attention.self\n",
        "            sout = blay.attention.output\n",
        "\n",
        "            query_out = satt.transpose_for_scores(satt.query(hidden_rep))\n",
        "            key_out = satt.transpose_for_scores(satt.key(hidden_rep))\n",
        "            value_out = satt.transpose_for_scores(satt.value(hidden_rep))\n",
        "\n",
        "            att_sc = torch.matmul(query_out, key_out.transpose(-1, -2)) # [32, 12, 128, 128], has all heads separated\n",
        "            att_sc = att_sc / math.sqrt(satt.attention_head_size)\n",
        "            att_sc += ext_att_mask # contains a bunch of negative infinities to remove stuff in softmax computation\n",
        "            att_prob = torch.nn.functional.softmax(att_sc, dim=-1)\n",
        "\n",
        "            ctxt = torch.matmul(att_prob, value_out) # [32, 12, 128, 64] still separated btwn heads\n",
        "            ctxt = ctxt.permute(0, 2, 1, 3).contiguous() # [32, 128, 12, 64]\n",
        "            sense_mask = mask_tensor[num_heads*i: num_heads*(i + 1)]\n",
        "\n",
        "            ctxt = ctxt * sense_mask[None, None, :, None] # add mask into the forward pass\n",
        "\n",
        "            new_shape = ctxt.size()[:-2] + (satt.all_head_size,)\n",
        "            ctxt = ctxt.view(*new_shape) # [32, 128, 768], THIS IS THE OUTPUT OF SELF ATTN\n",
        "\n",
        "            out = sout.dense(ctxt)\n",
        "            out = sout.LayerNorm(out + hidden_rep)\n",
        "\n",
        "            hidden_rep = apply_chunking_to_forward(blay.feed_forward_chunk,\n",
        "                    blay.chunk_size_feed_forward, blay.seq_len_dim, out)\n",
        "\n",
        "        output = BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=hidden_rep,\n",
        "                past_key_values=None, hidden_states=None, attentions=None,\n",
        "                cross_attentions=None)\n",
        "        pooled_output = model.bert.pooler(output[0])\n",
        "        pooled_output_wrapper = BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "                last_hidden_state=output[0], pooler_output=pooled_output,\n",
        "                past_key_values=None, hidden_states=None, attentions=None,\n",
        "                cross_attentions=None)\n",
        "        pool_out = pooled_output_wrapper[1]\n",
        "        logits = model.classifier(pool_out)\n",
        "        loss = loss_fn(logits.view(-1, model.num_labels), labels.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "    agg_grad = mask_tensor.grad.detach().cpu()\n",
        "    with torch.no_grad():\n",
        "        agg_grad = torch.abs(agg_grad)\n",
        "    imp_idxs = set(torch.argsort(agg_grad, descending=True)[:final_num_heads].cpu().tolist())\n",
        "    for l in range(num_layers):\n",
        "        heads_to_prune = []\n",
        "        for hi in range(num_heads):\n",
        "             curr_idx = num_heads * l + hi\n",
        "             if not curr_idx in imp_idxs:\n",
        "                 heads_to_prune.append(hi)\n",
        "        assert len(heads_to_prune) < num_heads\n",
        "        model.bert.encoder.layer[l].attention.prune_heads(heads_to_prune)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "IGDNa_9gyVQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prune model"
      ],
      "metadata": {
        "id": "XbdADMgGytn6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFsFSVBAxdKJ"
      },
      "outputs": [],
      "source": [
        "def prune_distilbert(args, model, dl, num_batches, num_iter, ratios, training_dict, val_arguments, per_layer_epochs, final_epochs, prune_type='ispasp++'):\n",
        "    # performs i-SpaSP pruning on the distilBERT model for squad\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # get the output dir\n",
        "    save_base = training_dict['args'].output_dir\n",
        "\n",
        "    transformer = model.distilbert.transformer\n",
        "    layers = transformer.layer\n",
        "\n",
        "    all_metrics = {}\n",
        "    metrics = validate(args, **val_arguments, device=device)\n",
        "    print(f'Pre-pruning metrics = {metrics}')\n",
        "    all_metrics['pre-pruning'] = metrics\n",
        "\n",
        "    S_dict = {}\n",
        "\n",
        "    # start passing thru each layer of encoder\n",
        "    if prune_type in ['random', 'ispasp', 'ispasp++']:\n",
        "        for i, rat in enumerate(ratios):\n",
        "            if i < args.min_layer or i > args.max_layer:\n",
        "              continue\n",
        "            if prune_type == 'ispasp++':\n",
        "                data_list = []\n",
        "                mask_list = []\n",
        "                for b in range(num_batches):\n",
        "                    data_in = next(iter(dl))\n",
        "                    with torch.no_grad():\n",
        "                        model.eval()\n",
        "                        hidden_rep, ext_att_mask = get_transformer_arguments(model, data_in, device)\n",
        "                        # hidden_rep, ext_att_mask = get_bert_embeddings(model, data_in)\n",
        "                        for j in range(i):\n",
        "                            hidden_rep = layers[j](hidden_rep, ext_att_mask)[0]\n",
        "                            #hidden_rep = bert_layer_forward(model.bert.encoder.layer[j],\n",
        "                            #        hidden_rep, ext_att_mask)\n",
        "                        data_list.append(hidden_rep.detach().cpu())\n",
        "                        mask_list.append(ext_att_mask.detach().cpu())\n",
        "                layers[i].attention, S = prune_attn_layer(args, layers[i].attention, data_list, mask_list, val_arguments, prune_ratio=rat, layer_id=i, device=device)\n",
        "            elif prune_type == 'ispasp':\n",
        "                data_list = []\n",
        "                mask_list = []\n",
        "                for b in range(num_batches):\n",
        "                    data_in = next(iter(dl))\n",
        "                    with torch.no_grad():\n",
        "                        model.eval()\n",
        "                        hidden_rep, ext_att_mask = get_transformer_arguments(model, data_in, device)\n",
        "                        # hidden_rep, ext_att_mask = get_bert_embeddings(model, data_in)\n",
        "                        for j in range(i):\n",
        "                            hidden_rep = layers[j](hidden_rep, ext_att_mask)[0]\n",
        "                            #hidden_rep = bert_layer_forward(model.bert.encoder.layer[j],\n",
        "                            #        hidden_rep, ext_att_mask)\n",
        "                        data_list.append(hidden_rep.detach().cpu())\n",
        "                        mask_list.append(ext_att_mask.detach().cpu())\n",
        "                layers[i], S = prune_layer_ispasp(layers[i], data_list, mask_list, num_iter=args.iters, prune_ratio=rat, layer_id=i)\n",
        "                # layers[i].attention = prune_attn_layer(args, layers[i].attention, data_list, mask_list, val_arguments, prune_ratio=rat, layer_id=i, device=device)\n",
        "            elif prune_type == 'random':\n",
        "                layers[i] = bert_layer_random_prune(\n",
        "                        layers[i], prune_ratio=rat)\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "            output_dir = os.path.join(save_base, f'prune_layer_{i}/')\n",
        "            model, layer_metrics = train_and_eval(args, model, training_dict, val_arguments, per_layer_epochs,\n",
        "                    save_path=output_dir, save_model=False, lr=5e-5, lr_decay=True, device=device)\n",
        "            all_metrics[f'layer {i}, before ft'] = layer_metrics[0]\n",
        "            all_metrics[f'layer {i}, after ft'] = layer_metrics[1]\n",
        "            S_dict[f'layer {i}'] = S\n",
        "\n",
        "        model, final_metrics = train_and_eval(args, model, training_dict, val_arguments, final_epochs,\n",
        "                save_path=save_base, save_model=True, lr=args.final_ft_lr, lr_decay=True, device=device)\n",
        "        all_metrics['final, before ft'] = final_metrics[0]\n",
        "        all_metrics['final, after ft'] = final_metrics[1]\n",
        "\n",
        "    elif prune_type == 'masking':\n",
        "        model = prune_global_masking(model, dl, num_batches=num_batches, ratio=ratios[0])\n",
        "        model = train_and_eval(args, model, training_dict, val_arguments, final_epochs,\n",
        "                save_path=save_base, save_model=True, lr=5e-5, lr_decay=True, device=device)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return all_metrics, S_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prune attention layer"
      ],
      "metadata": {
        "id": "E3VcmNnJ3Ce7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_attn_layer(args, dense_layer, hidden_states_list, mask_list, val_arguments, prune_ratio=0.5, layer_id=None, device='cpu'):\n",
        "  if layer_id is not None:\n",
        "    print(f'\\nPruning layer {layer_id}')\n",
        "\n",
        "  dense_layer.pruned_heads =set([])\n",
        "  n_heads_to_keep = int(dense_layer.n_heads * prune_ratio)\n",
        "\n",
        "  pruned_layer = deepcopy(dense_layer)\n",
        "  S = set([]) # heads to keep\n",
        "  S_indexer = None\n",
        "\n",
        "  qkv_optimizer, out_optimizer = None, None\n",
        "\n",
        "  Q = deepcopy(dense_layer)\n",
        "  dense_layer = dense_layer.to(device)\n",
        "\n",
        "  hidden_states = torch.cat(hidden_states_list, dim=0).to(device)\n",
        "  attn_mask = torch.cat(mask_list, dim=0).to(device)\n",
        "\n",
        "  for t in range(args.iters):\n",
        "    # pruned_layer.pruned_heads = set()\n",
        "    if args.validate_iter:\n",
        "      val_arguments['model'].distilbert.transformer.layer[layer_id].attention = pruned_layer # might not work with dimensions\n",
        "      acc = validate(args, **val_arguments, device=device)\n",
        "      pruned_layer = pruned_layer.to(device)\n",
        "\n",
        "    if len(S) > 0:\n",
        "      pruned_grad_dict, dense_grad_dict, loss = compute_grads(args, dense_layer, pruned_layer, hidden_states, S_indexer, attn_mask)\n",
        "    else:\n",
        "      dense_grad_dict, loss = compute_grads(args, dense_layer, pruned_layer, hidden_states, S_indexer, attn_mask)\n",
        "\n",
        "    if args.validate_iter:\n",
        "      print(f'Iteration {t}: S = {S} | Loss = {loss} | Exact Match = {acc[\"exact_match\"]} | F1 = {acc[\"f1\"]}')\n",
        "    elif args.iter_verbose:\n",
        "      print(f'Iteration {t}: S = {S} | Loss = {loss}')\n",
        "\n",
        "    if len(S) > 0:\n",
        "      Q = update_step(args, Q, pruned_grad_dict, S_indexer, device=device)\n",
        "\n",
        "    if args.trunc_strategy == 'magnitude':\n",
        "      importance = calc_importance_by_magnitude(args, Q, device=device)\n",
        "    elif args.trunc_strategy == 'Q attn scores':\n",
        "      importance = calc_importance_by_attn_scores(args, Q, hidden_states, attn_mask, device=device)\n",
        "    elif args.trunc_strategy == 'dense layer attn scores':\n",
        "      importance = calc_importance_by_attn_scores(args, dense_layer, hidden_states, attn_mask, device=device)\n",
        "    else:\n",
        "      raise NotImplementedError()\n",
        "\n",
        "    D = find_and_merge(dense_layer, dense_grad_dict, S, n_heads_to_keep)\n",
        "    pruned_layer, S, S_indexer = truncate(args, pruned_layer, Q, importance, D, n_heads_to_keep)\n",
        "    pruned_layer, Q = debias(args, pruned_layer, dense_layer, Q, args.debias_iters, hidden_states, attn_mask, S_indexer, head_mask=None, device=device)\n",
        "\n",
        "    del dense_grad_dict\n",
        "\n",
        "  print(f'Post-pruning: S = {S}')\n",
        "  # all_heads = set(range(dense_layer.n_heads))\n",
        "\n",
        "  # dense_layer.prune_heads(all_heads.difference(S))\n",
        "  # layer.out_lin = pruned_out_lin\n",
        "\n",
        "  if layer_id is not None:\n",
        "    print(f'Done pruning layer {layer_id}')\n",
        "\n",
        "  del Q, dense_layer\n",
        "  return pruned_layer, S"
      ],
      "metadata": {
        "id": "tT-9ixBXzs_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prune"
      ],
      "metadata": {
        "id": "_vgCXBPSHRgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "2WKY_r5_yic0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    model_args = ModelArguments(tokenizer_name=args.tokenizer_name,\n",
        "                                model_name_or_path=args.model_name)\n",
        "\n",
        "    data_args = DataTrainingArguments()\n",
        "\n",
        "    training_args = TrainingArguments(output_dir=args.output_dir,\n",
        "                                      do_train=True,\n",
        "                                      do_eval=False)\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "\n",
        "    log_level = training_args.get_process_log_level()\n",
        "    logger.setLevel(log_level)\n",
        "    datasets.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.enable_default_handler()\n",
        "    transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "    # Log on each process the small summary:\n",
        "    logger.warning(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu} \"\n",
        "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Detecting last checkpoint.\n",
        "    last_checkpoint = None\n",
        "\n",
        "    set_seed(args.random_seed)\n",
        "\n",
        "    if training_args.do_train:\n",
        "        train_dataset = load_dataset(\n",
        "            args.dataset_name, split=\"train\", cache_dir=model_args.cache_dir\n",
        "        )\n",
        "        train_dataset = train_dataset.shuffle(seed=args.random_seed)\n",
        "\n",
        "    if training_args.do_train:\n",
        "        val_dataset = load_dataset(\n",
        "            args.dataset_name, split=\"validation\", cache_dir=model_args.cache_dir\n",
        "        )\n",
        "        val_dataset = val_dataset.shuffle(seed=args.random_seed)\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "        # do_lower_case=model_args.do_lower_case,\n",
        "        # cache_dir=model_args.cache_dir,\n",
        "        # use_fast=model_args.use_fast_tokenizer,\n",
        "        # revision=model_args.model_revision,\n",
        "        # use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "\n",
        "    def preprocess_w_tokenizer(examples):\n",
        "      return preprocess_function(examples, tokenizer)\n",
        "\n",
        "    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
        "        train_dataset = train_dataset.select(range(args.train_size))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_w_tokenizer,\n",
        "            batched=True,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on train dataset\",\n",
        "        )\n",
        "\n",
        "    # Log a few random samples from the training set:\n",
        "    for index in random.sample(range(len(train_dataset)), 3):\n",
        "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    if args.validate_iter or args.validate_layer:\n",
        "      val_arguments = get_val_arguments(args, val_dataset, model, tokenizer)\n",
        "\n",
        "    data_collator = default_data_collator\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # prune the heads of each layer\n",
        "    train_dl = trainer.get_train_dataloader()\n",
        "\n",
        "    training_dict = {\n",
        "        'args': training_args,\n",
        "        'train_dataset': train_dataset,\n",
        "        'tokenizer': tokenizer,\n",
        "        'data_collator': data_collator,\n",
        "    }\n",
        "\n",
        "    if not args.iterative:\n",
        "        ratios = [args.total_prune_ratio for x in range(6)]\n",
        "        result_metrics, S_dict = prune_distilbert(args, model, train_dl, args.num_batches, args.iters, ratios,\n",
        "                training_dict, val_arguments, per_layer_epochs=args.epochs_per_layer, final_epochs=args.final_epochs, prune_type=args.prune_type)\n",
        "    else:\n",
        "        result_metrics = []\n",
        "        ratio_to_elim = 1.0 - args.total_prune_ratio\n",
        "        prune_iters = int(ratio_to_elim / args.ratio_per_iter)\n",
        "        print(f'\\nRunning {prune_iters} pruning iterations\\n')\n",
        "        for pt in range(prune_iters):\n",
        "            curr_ratio = 1.0 - (args.ratio_per_iter * (pt + 1))\n",
        "            prev_ratio = float(int((1.0 - args.ratio_per_iter * pt)*12.0) / 12.0)\n",
        "            new_ratio = curr_ratio / prev_ratio\n",
        "            ratios = [new_ratio for x in range(6)]\n",
        "            result_metrics.append(prune_distilbert(args, model, train_dl, args.num_batches, args.iters, ratios,\n",
        "                    training_dict, val_arguments, per_layer_epochs=args.epochs_per_layer, final_epochs=args.final_epochs, prune_type=args.prune_type))\n",
        "\n",
        "    return result_metrics, S_dict"
      ],
      "metadata": {
        "id": "i2RxlOvLyDLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run"
      ],
      "metadata": {
        "id": "y53kjzW1AKiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'output_dir': '/content',\n",
        "    'results_dir': '/content/results/',\n",
        "    'exp_num': 0,\n",
        "\n",
        "    'tokenizer_name': 'distilbert-base-uncased', #'distilbert-base-uncased-distilled-squad',\n",
        "    'model_name': 'distilbert-base-uncased',\n",
        "    'dataset_name': 'squad',\n",
        "\n",
        "    'random_seed': 0,\n",
        "\n",
        "    'train_size': 80000,\n",
        "    'val_size': 25,\n",
        "\n",
        "    'prune_type': 'ispasp++',\n",
        "\n",
        "    'final_epochs': 0.1,\n",
        "    'epochs_per_layer': 0,\n",
        "    'freeze_encoder': True,\n",
        "    'final_ft_lr': 1e-2,\n",
        "\n",
        "    'num_batches': 5,\n",
        "    'iters': 5,\n",
        "    'total_prune_ratio': 0.3,\n",
        "    'iterative': True,\n",
        "    'ratio_per_iter': 0.1,\n",
        "    'min_layer': -1,\n",
        "    'max_layer': 0,\n",
        "\n",
        "    'validate_iter': False,\n",
        "    'validate_layer': True,\n",
        "    'iter_verbose': True,\n",
        "    'verbose': False,\n",
        "\n",
        "    'dense_update': True,\n",
        "    'maintain_Q': True,\n",
        "    'eta': 5e-5,\n",
        "    'debias_eta': 5e-5,\n",
        "    'debias_iters': 5,\n",
        "    'debias_dense': False,\n",
        "    'trunc_strategy': 'dense layer attn scores', # Q attn scores, dense layer attn scores, magnitude, dense grad,\n",
        "    'choose_from_D': False,\n",
        "\n",
        "}\n",
        "d_args = dotdict(args)"
      ],
      "metadata": {
        "id": "KxUd_0oE5Nym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for run in range(1):\n",
        "  torch.cuda.empty_cache()\n",
        "  run_mets, S_dict = main(d_args)\n",
        "  torch.cuda.empty_cache()\n",
        "  d_args.random_seed += 1"
      ],
      "metadata": {
        "id": "EnXRi-MYVd4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save results\n"
      ],
      "metadata": {
        "id": "MDyF7CNo9arX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "while True:\n",
        "  try:\n",
        "    os.mkdir(f\"exp_{d_args.exp_num}_{d_args.prune_type}\")\n",
        "  except FileExistsError:\n",
        "    d_args.exp_num += 1\n",
        "    continue\n",
        "  break\n",
        "\n",
        "S_dict_json = {name: list(S_dict[name]) for name in S_dict}\n",
        "results_dict = {'metrics': run_mets, 'S': S_dict_json, 'args': args}\n",
        "results_json = json.dumps(results_dict, indent=4)"
      ],
      "metadata": {
        "id": "YPOIoB-D81IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"exp_{d_args.exp_num}_{d_args.prune_type}/results.json\", \"w\") as outfile:\n",
        "    outfile.write(results_json)\n",
        "\n",
        "with open(f\"/content/gdrive/MyDrive/Research/Results/distilBERT_ispasp++/exp_{d_args.exp_num}_{d_args.prune_type}_results.json\", \"w\") as outfile:\n",
        "    outfile.write(results_json)"
      ],
      "metadata": {
        "id": "upVqE833dF5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plot results"
      ],
      "metadata": {
        "id": "wh-Sx5rE9dYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_by_layer = results_dict['metrics']\n",
        "layer_names = results_by_layer.keys()\n",
        "metric_names = results_by_layer['pre-pruning'].keys()\n",
        "\n",
        "run_metrics = {met: [0 for _ in layer_names] for met in metric_names}\n",
        "for i, layer in enumerate(layer_names):\n",
        "  for met in metric_names:\n",
        "    run_metrics[met][i] = results_by_layer[layer][met]"
      ],
      "metadata": {
        "id": "yoLhmTJllG6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for metric in run_metrics.keys():\n",
        "  plt.plot(run_metrics[metric], label=metric)\n",
        "plt.xlabel('Point in Algorithm')\n",
        "plt.xticks(ticks=range(len(layer_names)), labels=layer_names, rotation=45, ha='right')\n",
        "plt.title('Results with 2000 finetuning steps per layer')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JgUOFmay9iAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
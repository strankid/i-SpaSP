{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "from quant import *\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "DEBUG = True \n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "\n",
    "class SparseGPT:\n",
    "\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.dev = self.layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.rows = W.shape[0]\n",
    "        self.columns = W.shape[1]\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "\n",
    "    def add_batch(self, inp, out, blocksize=1024):\n",
    "        if DEBUG:\n",
    "            self.inp1 = inp\n",
    "            self.out1 = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        inp = math.sqrt(2 / self.nsamples) * inp.float()\n",
    "        self.H += inp.matmul(inp.t())\n",
    "\n",
    "    def fasterprune(\n",
    "        self, sparsity, prunen=0, prunem=0, blocksize=128, percdamp=.01\n",
    "    ):\n",
    "        W = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        W = W.float()\n",
    "\n",
    "        if hasattr(self, 'quantizer'):\n",
    "            if not self.quantizer.ready():\n",
    "                self.quantizer.find_params(W, weight=True)\n",
    "\n",
    "        tick = time.time()\n",
    "\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "\n",
    "        Losses = torch.zeros(self.rows, device=self.dev)\n",
    "\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        diag = torch.arange(self.columns, device=self.dev)\n",
    "        H[diag, diag] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "\n",
    "        mask = None\n",
    "\n",
    "        for i1 in range(0, self.columns, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.columns)\n",
    "            count = i2 - i1\n",
    "\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "\n",
    "            if prunen == 0: \n",
    "                if mask is not None:\n",
    "                    mask1 = mask[:, i1:i2]\n",
    "                else:\n",
    "                    tmp = W1 ** 2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n",
    "                    thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n",
    "                    mask1 = tmp <= thresh\n",
    "            else:\n",
    "                mask1 = torch.zeros_like(W1) == 1\n",
    "\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = Hinv1[i, i]\n",
    "\n",
    "                if prunen != 0 and i % prunem == 0:\n",
    "                    tmp = W1[:, i:(i + prunem)] ** 2 / (torch.diag(Hinv1)[i:(i + prunem)].reshape((1, -1))) ** 2\n",
    "                    mask1.scatter_(1, i + torch.topk(tmp, prunen, dim=1, largest=False)[1], True)\n",
    "\n",
    "                q = w.clone()\n",
    "                q[mask1[:, i]] = 0\n",
    "\n",
    "                if hasattr(self, 'quantizer'):\n",
    "                    q = quantize(\n",
    "                        q.unsqueeze(1), self.quantizer.scale, self.quantizer.zero, self.quantizer.maxq\n",
    "                    ).flatten()\n",
    "\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q) ** 2 / d ** 2\n",
    "\n",
    "                err1 = (w - q) / d\n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "\n",
    "            W[:, i1:i2] = Q1\n",
    "            Losses += torch.sum(Losses1, 1) / 2\n",
    "\n",
    "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
    "\n",
    "            if DEBUG:\n",
    "                self.layer.weight.data[:, :i2] = W[:, :i2]\n",
    "                self.layer.weight.data[:, i2:] = W[:, i2:]\n",
    "                print(\"error norm\", torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "                # print(torch.sum(Losses))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print('time %.2f' % (time.time() - tick))\n",
    "        print('error', torch.sum(Losses).item())\n",
    "\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.layer.weight.data = W.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n",
    "        if DEBUG:\n",
    "            print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "\n",
    "    def free(self):\n",
    "        if DEBUG:\n",
    "            self.inp1 = None\n",
    "            self.out1 = None\n",
    "        self.H = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, trainloader, epochs = 4):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return 100 * correct // total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: plane is 49.7 %\n",
      "Accuracy for class: car   is 59.4 %\n",
      "Accuracy for class: bird  is 51.3 %\n",
      "Accuracy for class: cat   is 44.9 %\n",
      "Accuracy for class: deer  is 51.0 %\n",
      "Accuracy for class: dog   is 26.3 %\n",
      "Accuracy for class: frog  is 74.5 %\n",
      "Accuracy for class: horse is 55.7 %\n",
      "Accuracy for class: ship  is 73.0 %\n",
      "Accuracy for class: truck is 59.1 %\n"
     ]
    }
   ],
   "source": [
    "# # prepare to count predictions for each class\n",
    "# correct_pred = {classname: 0 for classname in classes}\n",
    "# total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# # again no gradients needed\n",
    "# with torch.no_grad():\n",
    "#     for data in testloader:\n",
    "#         images, labels = data\n",
    "#         outputs = net(images)\n",
    "#         _, predictions = torch.max(outputs, 1)\n",
    "#         # collect the correct predictions for each class\n",
    "#         for label, prediction in zip(labels, predictions):\n",
    "#             if label == prediction:\n",
    "#                 correct_pred[classes[label]] += 1\n",
    "#             total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# # print accuracy for each class\n",
    "# for classname, correct_count in correct_pred.items():\n",
    "#     accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "#     print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruneLayers(model, trainloader, sparsity=0.5, blocksize=56):\n",
    "    batch, _ = next(iter(trainloader))\n",
    "    inp1 = model.pool(F.relu(model.conv1(batch)))\n",
    "    inp1 = model.pool(F.relu(model.conv2(inp1)))\n",
    "    inp1 = torch.flatten(inp1, 1) \n",
    "    out1 = model.fc1(inp1)\n",
    "    \n",
    "    sparseLayer1 = SparseGPT(model.fc1)\n",
    "    sparseLayer1.add_batch(inp1, out1)\n",
    "    sparseLayer1.fasterprune(sparsity, blocksize=blocksize)\n",
    "    \n",
    "    inp2 = F.relu(out1)\n",
    "    out2 = model.fc2(inp2)\n",
    "    sparseLayer2 = SparseGPT(model.fc2)\n",
    "    sparseLayer2.add_batch(inp2, out2)\n",
    "    sparseLayer2.fasterprune(sparsity, blocksize=blocksize)\n",
    "\n",
    "    inp3 = F.relu(out2)\n",
    "    out3 = model.fc3(inp3)\n",
    "    sparseLayer3 = SparseGPT(model.fc3)\n",
    "    sparseLayer3.add_batch(inp3, out3)\n",
    "    sparseLayer3.fasterprune(sparsity, blocksize=blocksize)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "denseModel = Net()\n",
    "train_model(denseModel, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error norm tensor(1.4551e-05, grad_fn=<SumBackward0>)\n",
      "error norm tensor(4.1059e-05, grad_fn=<SumBackward0>)\n",
      "error norm tensor(8.6534e-05, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0002, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0005, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0128, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0271, grad_fn=<SumBackward0>)\n",
      "time 0.11\n",
      "error 0.09782558679580688\n",
      "tensor(0.0271, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0., grad_fn=<SumBackward0>)\n",
      "error norm tensor(0., grad_fn=<SumBackward0>)\n",
      "error norm tensor(0., grad_fn=<SumBackward0>)\n",
      "time 0.03\n",
      "error 0.0\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "error norm tensor(9.6443e-06, grad_fn=<SumBackward0>)\n",
      "error norm tensor(2.6978e-05, grad_fn=<SumBackward0>)\n",
      "time 0.02\n",
      "error 0.00015530409291386604\n",
      "tensor(2.6978e-05, grad_fn=<SumBackward0>)\n",
      "error norm tensor(9.5930e-05, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0006, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0017, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0030, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1208, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.2257, grad_fn=<SumBackward0>)\n",
      "time 0.11\n",
      "error 0.6562166213989258\n",
      "tensor(0.2257, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "time 0.03\n",
      "error 0.006515092682093382\n",
      "tensor(0.0004, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0011, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0080, grad_fn=<SumBackward0>)\n",
      "time 0.02\n",
      "error 0.017424412071704865\n",
      "tensor(0.0080, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0003, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0008, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0017, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0035, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0047, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.5796, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.7863, grad_fn=<SumBackward0>)\n",
      "time 0.10\n",
      "error 2.1827330589294434\n",
      "tensor(0.7863, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0051, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0364, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0434, grad_fn=<SumBackward0>)\n",
      "time 0.03\n",
      "error 0.15633022785186768\n",
      "tensor(0.0434, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0048, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0510, grad_fn=<SumBackward0>)\n",
      "time 0.01\n",
      "error 0.0977783352136612\n",
      "tensor(0.0510, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0037, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0170, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0231, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0410, grad_fn=<SumBackward0>)\n",
      "error norm tensor(2.7110, grad_fn=<SumBackward0>)\n",
      "error norm tensor(3.4196, grad_fn=<SumBackward0>)\n",
      "time 0.10\n",
      "error 8.26060962677002\n",
      "tensor(3.4196, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0315, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.4210, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.6308, grad_fn=<SumBackward0>)\n",
      "time 0.03\n",
      "error 1.4087073802947998\n",
      "tensor(0.6308, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0295, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.2380, grad_fn=<SumBackward0>)\n",
      "time 0.02\n",
      "error 0.4561725854873657\n",
      "tensor(0.2380, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0012, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0041, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0106, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0232, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0307, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0602, grad_fn=<SumBackward0>)\n",
      "error norm tensor(6.2737, grad_fn=<SumBackward0>)\n",
      "error norm tensor(8.6062, grad_fn=<SumBackward0>)\n",
      "time 0.10\n",
      "error 15.772558212280273\n",
      "tensor(8.6062, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0657, grad_fn=<SumBackward0>)\n",
      "error norm tensor(1.6902, grad_fn=<SumBackward0>)\n",
      "error norm tensor(2.9559, grad_fn=<SumBackward0>)\n",
      "time 0.03\n",
      "error 5.3182268142700195\n",
      "tensor(2.9559, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0478, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.5136, grad_fn=<SumBackward0>)\n",
      "time 0.02\n",
      "error 0.8709613680839539\n",
      "tensor(0.5136, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0031, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0110, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0236, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0461, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0618, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1248, grad_fn=<SumBackward0>)\n",
      "error norm tensor(18.3105, grad_fn=<SumBackward0>)\n",
      "error norm tensor(32.1226, grad_fn=<SumBackward0>)\n",
      "time 0.10\n",
      "error 47.417259216308594\n",
      "tensor(32.1226, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1978, grad_fn=<SumBackward0>)\n",
      "error norm tensor(5.1500, grad_fn=<SumBackward0>)\n",
      "error norm tensor(10.4222, grad_fn=<SumBackward0>)\n",
      "time 0.03\n",
      "error 15.408839225769043\n",
      "tensor(10.4222, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1986, grad_fn=<SumBackward0>)\n",
      "error norm tensor(2.8730, grad_fn=<SumBackward0>)\n",
      "time 0.02\n",
      "error 3.8089182376861572\n",
      "tensor(2.8730, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0039, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0145, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0382, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0767, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1018, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1680, grad_fn=<SumBackward0>)\n",
      "error norm tensor(40.8138, grad_fn=<SumBackward0>)\n",
      "error norm tensor(104.6462, grad_fn=<SumBackward0>)\n",
      "time 0.11\n",
      "error 125.24073791503906\n",
      "tensor(104.6462, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.2553, grad_fn=<SumBackward0>)\n",
      "error norm tensor(9.8430, grad_fn=<SumBackward0>)\n",
      "error norm tensor(21.8505, grad_fn=<SumBackward0>)\n",
      "time 0.02\n",
      "error 28.240123748779297\n",
      "tensor(21.8505, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.3281, grad_fn=<SumBackward0>)\n",
      "error norm tensor(6.5956, grad_fn=<SumBackward0>)\n",
      "time 0.16\n",
      "error 7.862961292266846\n",
      "tensor(6.5956, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0071, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0241, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0540, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1359, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1960, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.3452, grad_fn=<SumBackward0>)\n",
      "error norm tensor(153.6042, grad_fn=<SumBackward0>)\n",
      "error norm tensor(299.9431, grad_fn=<SumBackward0>)\n",
      "time 0.10\n",
      "error 336.1209716796875\n",
      "tensor(299.9431, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.5329, grad_fn=<SumBackward0>)\n",
      "error norm tensor(33.0558, grad_fn=<SumBackward0>)\n",
      "error norm tensor(98.0178, grad_fn=<SumBackward0>)\n",
      "time 0.04\n",
      "error 109.61137390136719\n",
      "tensor(98.0178, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.8731, grad_fn=<SumBackward0>)\n",
      "error norm tensor(51.6723, grad_fn=<SumBackward0>)\n",
      "time 0.01\n",
      "error 53.70086669921875\n",
      "tensor(51.6723, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0113, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.0520, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.1374, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.4138, grad_fn=<SumBackward0>)\n",
      "error norm tensor(0.6482, grad_fn=<SumBackward0>)\n",
      "error norm tensor(1.4265, grad_fn=<SumBackward0>)\n",
      "error norm tensor(376.9910, grad_fn=<SumBackward0>)\n",
      "error norm tensor(1158.6084, grad_fn=<SumBackward0>)\n",
      "time 0.09\n",
      "error 1211.19482421875\n",
      "tensor(1158.6084, grad_fn=<SumBackward0>)\n",
      "error norm tensor(1.1304, grad_fn=<SumBackward0>)\n",
      "error norm tensor(120.1575, grad_fn=<SumBackward0>)\n",
      "error norm tensor(453.4139, grad_fn=<SumBackward0>)\n",
      "time 0.03\n",
      "error 471.6040344238281\n",
      "tensor(453.4139, grad_fn=<SumBackward0>)\n",
      "error norm tensor(2.7730, grad_fn=<SumBackward0>)\n",
      "error norm tensor(159.8617, grad_fn=<SumBackward0>)\n",
      "time 0.02\n",
      "error 164.28933715820312\n",
      "tensor(159.8617, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "modelAccuracy = [validate_model(denseModel, testloader)]\n",
    "\n",
    "for sparsity in range(1,10):\n",
    "    sparseModel = copy.deepcopy(denseModel)\n",
    "    pruneLayers(sparseModel, trainloader, sparsity/10)\n",
    "    modelAccuracy.append(validate_model(sparseModel, testloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e4e01b2c70>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([x/10 for x in range(0,10)], modelAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9001666666666667, 0.9000992063492064, 0.9023809523809524)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity1 = sparseModel.fc1.weight.data[sparseModel.fc1.weight.data == 0].shape[0] / (sparseModel.fc1.weight.data.shape[0] * sparseModel.fc1.weight.data.shape[1]) \n",
    "sparsity2 = sparseModel.fc2.weight.data[sparseModel.fc2.weight.data == 0].shape[0] / (sparseModel.fc2.weight.data.shape[0] * sparseModel.fc2.weight.data.shape[1]) \n",
    "sparsity3 = sparseModel.fc3.weight.data[sparseModel.fc3.weight.data == 0].shape[0] / (sparseModel.fc3.weight.data.shape[0] * sparseModel.fc3.weight.data.shape[1]) \n",
    "sparsity1, sparsity2, sparsity3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.1566,  0.0000,  0.0000,  0.1963,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2323,  0.4301, -0.1493,\n",
       "          0.0000,  0.0000, -0.2193,  0.0000,  0.0000,  0.0000,  0.5497,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.6138,  0.0000,  0.0000,  0.0000,  0.3898,\n",
       "          0.0000,  0.0000,  0.0000, -0.1234],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.2872,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2679,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.3139,  0.0000,  0.0000, -0.3259,  0.0000,  0.2838,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.6094,  0.0000,  0.0000,  0.0000,  0.5517,\n",
       "          0.0000, -0.7465,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.2493,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3538,\n",
       "          0.0000, -0.4395,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3145,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.3646,  0.0000, -0.6875,  0.0000,  0.2640,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.2016,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2230,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.3215,\n",
       "          0.0000,  0.2817,  0.0000,  0.7978],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.2175,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2613,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -0.3081,  0.0000,  0.0000,  0.0000,  0.3222,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.5952,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.4613],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4001,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.1851,  0.0000, -0.6471,  0.0000, -0.2051,\n",
       "          0.0000,  0.0000,  0.0000,  1.2555],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2362,  0.0000,  0.0000,\n",
       "         -0.2790,  0.1573,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2848,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.2199,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -0.2833,  0.0000,  0.0000,  0.2339, -0.1973,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.4471,  0.0000,  0.0000,  0.0000, -0.0574,\n",
       "          0.0000, -0.3291,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.1887,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.2463,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2443,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -0.1907,  0.0000,  0.0000, -0.3895,  0.0000,\n",
       "          0.0000,  0.0000,  0.3050,  0.0000,  0.0000, -0.1108,  0.2065,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.5895,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2696,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2570,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2767,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0985,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.1935,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0860,\n",
       "          0.0000,  0.6185,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -1.0518,  0.0000,  0.0000,  0.0000,  0.1746,\n",
       "          0.0000,  0.1618,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.2779,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2125, -0.2379,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2474,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1161, -0.3012,  0.0000,\n",
       "          0.0000, -0.1757,  0.3133,  0.0000,  0.0000,  0.0000,  0.1601,  0.4524,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000, -1.1430]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseModel.fc3.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
